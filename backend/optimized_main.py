"""
FastAPI Web Server for Trading Dashboard API

This is the FRONTEND API SERVER, not a scanner. It provides:
- HTTP REST endpoints for dashboard data (instruments, options chains, alerts)
- WebSocket connections for real-time updates
- Authentication and rate limiting
- Serves alerts generated by scanner_main.py from Redis

Architecture:
- scanner_main.py → Processes market data → Generates alerts → Stores in Redis
- optimized_main.py ← Reads from Redis ← Serves to dashboard/frontend

Usage:
    uvicorn backend.optimized_main:app --host 0.0.0.0 --port 8000

Note: This is separate from scanner_main.py which is the actual market scanner.
"""

import os
import sys
from pathlib import Path

# ============================================================================
# PATH SETUP: Add project roots to sys.path for imports
# ============================================================================
# Directory structure:
#   aion_algo_trading/                    (project_root)
#   ├── redis_files/                     (redis_files module here)
#   ├── alerts/                           (alerts module here - in project_root)
#   ├── zerodha_websocket/                (zerodha_websocket - contains crawlers/)
#   │   └── crawlers/                      (crawlers module here)
#   ├── intraday_trading/                 (intraday_trading_root)
#   │   ├── utils/                        (utils module here)
#   │   ├── patterns/                     (patterns module here)
#   │   └── intraday_scanner/             (intraday_scanner module here)
#   └── aion_trading_dashboard/           (dashboard_root)
#       └── backend/
#           └── optimized_main.py         (this file)
#
# We need to add project_root to sys.path so redis_files, alerts can be imported.
# We need to add zerodha_websocket to sys.path so crawlers can be imported.
# We need to add intraday_trading_root to sys.path so utils/patterns/intraday_scanner can be imported.
# ============================================================================

# Get absolute paths
backend_dir = Path(__file__).resolve().parent  # backend/
dashboard_root = backend_dir.parent  # aion_trading_dashboard/
project_root = dashboard_root.parent  # aion_algo_trading/
intraday_trading_root = project_root / "intraday_trading"
zerodha_websocket_root = project_root / "zerodha_websocket"

# Verify paths exist
if not project_root.exists():
    raise RuntimeError(f"Project root not found: {project_root}")
if not intraday_trading_root.exists():
    raise RuntimeError(f"Intraday trading root not found: {intraday_trading_root}")

# Verify critical modules exist
redis_files_path = project_root / "redis_files"
if not redis_files_path.exists():
    raise RuntimeError(f"redis_files directory not found: {redis_files_path}")

alerts_path = project_root / "alerts"
if not alerts_path.exists():
    raise RuntimeError(f"alerts directory not found: {alerts_path}")

# Verify crawlers exists (in zerodha_websocket)
if zerodha_websocket_root.exists():
    crawlers_path = zerodha_websocket_root / "crawlers"
    if not crawlers_path.exists():
        print(f"⚠️  Warning: crawlers directory not found in {zerodha_websocket_root}")

# Add paths to sys.path (insert at beginning for priority)
# Order matters: more specific paths first, then general ones
# IMPORTANT: Insert AFTER venv site-packages to ensure venv packages are used first
paths_to_add = [
    str(intraday_trading_root),      # For utils/patterns/intraday_scanner imports (most specific)
    str(zerodha_websocket_root),     # For crawlers imports (if exists)
    str(project_root),                # For redis_files, alerts imports (general)
    str(dashboard_root),              # For backend imports (backward compatibility)
]

# Find venv site-packages index to insert after it
venv_site_packages_idx = None
for i, path in enumerate(sys.path):
    if '.venv' in path and 'site-packages' in path:
        venv_site_packages_idx = i
        break

for path in paths_to_add:
    if path not in sys.path and Path(path).exists():
        if venv_site_packages_idx is not None:
            # Insert after venv site-packages to prioritize venv packages
            sys.path.insert(venv_site_packages_idx + 1, path)
        else:
            # Fallback: insert at beginning if venv not found
            sys.path.insert(0, path)

# Type checking: Help linters understand the path setup
if "TYPE_CHECKING" in os.environ or os.getenv("MYPY_RUNNING"):
    # This helps type checkers understand the module structure
    pass

from fastapi import FastAPI, Request, Response, WebSocket, WebSocketDisconnect, Query, HTTPException, Depends, Header, Form, Body



@app.post("/api/orders/place")
@limiter.limit("10/minute")
async def place_order(
    request: Request,
    order_data: Dict = Body(...),
    current_user: dict = Depends(get_current_user)
):
    """Place a real order using the connected broker API."""
    try:
        user_id = current_user.get("username")
        if not user_id:
            raise HTTPException(status_code=401, detail="Invalid user")

        required_fields = ["symbol_code", "quantity", "side", "broker"]
        for field in required_fields:
            if not order_data.get(field):
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")

        from intraday_scanner.order_manager import get_order_manager
        order_manager = get_order_manager()

        result = await order_manager.place_order(user_id, order_data)
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error", "Order placement failed"))

        await store_user_order(user_id, result, order_data)

        return {
            "success": True,
            "order_id": result["order_id"],
            "broker_order_id": result.get("broker_order_id"),
            "message": result.get("message", "Order placed successfully"),
        }
    except HTTPException:
        raise
    except Exception as exc:
        print(f"❌ Order placement error: {exc}")
        raise HTTPException(status_code=500, detail=f"Order placement failed: {exc}")


async def store_user_order(user_id: str, order_result: Dict, order_data: Dict):
    """Persist order metadata for the user."""
    try:
        from redis_files.redis_client import RedisClientFactory

        db0_client = RedisClientFactory.get_system_client()
        order_id = order_result.get("order_id")
        if not order_id:
            return

        record_key = f"user_orders:{user_id}:{order_id}"
        order_record = {
            "user_id": user_id,
            "order_id": order_id,
            "broker_order_id": order_result.get("broker_order_id"),
            "symbol_code": order_data.get("symbol_code"),
            "quantity": order_data.get("quantity"),
            "side": order_data.get("side"),
            "order_type": order_data.get("order_type", "LIMIT"),
            "product": order_data.get("product", order_data.get("product_type", "MIS")),
            "price": order_data.get("price"),
            "is_amo": order_data.get("is_amo", False),
            "broker": order_data.get("broker"),
            "status": order_result.get("status", "PLACED"),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": order_result.get("message", ""),
        }

        db0_client.setex(record_key, 86400 * 7, json.dumps(order_record))

        user_orders_key = f"user_orders:{user_id}"
        db0_client.lpush(user_orders_key, order_id)
        db0_client.ltrim(user_orders_key, 0, 99)
    except Exception as exc:
        print(f"⚠️ Failed to store order for {user_id}: {exc}")


@app.get("/api/orders")
@limiter.limit("10/minute")
async def get_orders(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Return recent orders for the current user."""
    try:
        from redis_files.redis_client import RedisClientFactory
        user_id = current_user.get("username")
        db0_client = RedisClientFactory.get_system_client()
        order_ids = db0_client.lrange(f"user_orders:{user_id}", 0, 99) or []
        orders = []
        for raw_id in order_ids:
            order_id = raw_id.decode() if isinstance(raw_id, bytes) else raw_id
            record = db0_client.get(f"user_orders:{user_id}:{order_id}")
            if record:
                orders.append(json.loads(record))
        return {"success": True, "data": orders}
    except HTTPException:
        raise
    except Exception as exc:
        print(f"❌ Failed to retrieve orders: {exc}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve orders: {exc}")


@app.get("/api/orders/{order_id}")
@limiter.limit("20/minute")
async def get_order_status(
    request: Request,
    order_id: str,
    current_user: dict = Depends(get_current_user)
):
    """Fetch a specific order record for the user."""
    try:
        from redis_files.redis_client import RedisClientFactory
        user_id = current_user.get("username")
        db0_client = RedisClientFactory.get_system_client()
        record = db0_client.get(f"user_orders:{user_id}:{order_id}")
        if not record:
            raise HTTPException(status_code=404, detail=f"Order {order_id} not found")
        return {"success": True, "data": json.loads(record)}
    except HTTPException:
        raise
    except Exception as exc:
        print(f"❌ Failed to retrieve order status: {exc}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve order status: {exc}")


# ============================================================================
# BROKER ENDPOINTS (Frontend-facing)
# ============================================================================

@app.get("/api/brokers/status")
@limiter.limit("30/minute")
async def get_brokers_status(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Get broker connection status for current user"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    connections = user_data.get("brokerage_connections", {})
    statuses = []
    
    # Check Zerodha
    zerodha_data = connections.get("zerodha", {})
    statuses.append({
        "broker": "ZERODHA",
        "status": "ACTIVE" if zerodha_data.get("connected", False) else "DISCONNECTED",
        "expires_at": zerodha_data.get("expires_at"),
        "last_error": zerodha_data.get("last_error")
    })
    
    # Check Angel One
    angel_one_data = connections.get("angel_one", {})
    statuses.append({
        "broker": "ANGEL_ONE",
        "status": "ACTIVE" if angel_one_data.get("connected", False) else "DISCONNECTED",
        "expires_at": angel_one_data.get("expires_at"),
        "last_error": angel_one_data.get("last_error")
    })
    
    return statuses

@app.post("/api/brokers/zerodha/token")
@limiter.limit("10/minute")
async def connect_zerodha_token(
    request: Request,
    access_token: str = Form(...),
    current_user: dict = Depends(get_current_user)
):
    """Connect Zerodha using access token"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    api_key = os.getenv("ZERODHA_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Zerodha integration not configured")
    
    from kiteconnect import KiteConnect  # local import to avoid optional dependency during startup
    kite = KiteConnect(api_key=api_key)
    access_token_clean = access_token.strip()
    try:
        kite.set_access_token(access_token_clean)
        profile = kite.profile()
    except Exception as exc:
        logger.error(f"Zerodha token verification failed for {username}: {exc}")
        raise HTTPException(status_code=400, detail=f"Failed to verify Zerodha token: {exc}")
    
    brokerage_connections = user_data.get("brokerage_connections", {})
    expires_at = (datetime.now(timezone.utc) + timedelta(days=1)).isoformat()
    brokerage_connections["zerodha"] = {
        "connected": True,
        "connected_at": datetime.now(timezone.utc).isoformat(),
        "last_verified": datetime.now(timezone.utc).isoformat(),
        "expires_at": expires_at,
        "access_token": access_token_clean,
        "user_id": profile.get("user_id"),
        "user_name": profile.get("user_name"),
        "note": "Token verified successfully"
    }
    user_data["brokerage_connections"] = brokerage_connections
    
    from redis_files.redis_client import RedisClientFactory
    system_client = RedisClientFactory.get_system_client()
    system_client.set(f"user:{username}", json.dumps(user_data))
    
    return {
        "broker": "ZERODHA",
        "status": "ACTIVE",
        "expires_at": expires_at,
        "last_error": None
    }

@app.post("/api/brokers/angelone/token")
@limiter.limit("10/minute")
async def connect_angelone_token(
    request: Request,
    access_token: str = Form(...),
    client_code: str = Form(...),
    current_user: dict = Depends(get_current_user)
):
    """Connect Angel One using access token"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    client_code_clean = client_code.strip()
    if not client_code_clean:
        raise HTTPException(status_code=400, detail="Client code is required for Angel One")
    
    api_key = os.getenv("ANGEL_ONE_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Angel One integration not configured")
    
    try:
        from smartapi import SmartConnect
    except ImportError as exc:
        raise HTTPException(status_code=500, detail=f"Angel One SDK not available: {exc}")
    
    angel = SmartConnect(api_key=api_key)
    access_token_clean = access_token.strip()
    try:
        angel.set_session_token(access_token_clean, client_code_clean)
        # Lightweight verification call (LTP for NIFTY 50)
        angel.ltpData("NSE", "NIFTY 50", "99926000")
    except Exception as exc:
        logger.error(f"Angel One token verification failed for {username}: {exc}")
        raise HTTPException(status_code=400, detail=f"Failed to verify Angel One token: {exc}")
    
    brokerage_connections = user_data.get("brokerage_connections", {})
    expires_at = (datetime.now(timezone.utc) + timedelta(days=1)).isoformat()
    brokerage_connections["angel_one"] = {
        "connected": True,
        "connected_at": datetime.now(timezone.utc).isoformat(),
        "last_verified": datetime.now(timezone.utc).isoformat(),
        "expires_at": expires_at,
        "client_code": client_code_clean,
        "access_token": access_token_clean,
        "note": "Token verified successfully"
    }
    user_data["brokerage_connections"] = brokerage_connections
    
    from redis_files.redis_client import RedisClientFactory
    system_client = RedisClientFactory.get_system_client()
    system_client.set(f"user:{username}", json.dumps(user_data))
    
    return {
        "broker": "ANGEL_ONE",
        "status": "ACTIVE",
        "expires_at": expires_at,
        "last_error": None
    }

# ============================================================================
# AUTO-TRADE ENDPOINTS (My Algos)
# ============================================================================

@app.get("/api/auto_trade/rules")
@limiter.limit("30/minute")
async def get_auto_trade_rules(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Get auto-trade strategy rules for current user"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    # Load pattern registry to get available patterns
    pattern_registry_path = project_root / "intraday_trading" / "patterns" / "data" / "pattern_registry_config.json"
    patterns = []
    
    try:
        if pattern_registry_path.exists():
            with open(pattern_registry_path, 'r') as f:
                registry = json.load(f)
            
            # Get user's saved rules
            user_rules = user_data.get("auto_trade_rules", {})
            
            # Build strategy rules from registry
            for category_name, category_data in registry.get("categories", {}).items():
                for pattern_type in category_data.get("patterns", []):
                    pattern_config = registry.get("pattern_configs", {}).get(pattern_type, {})
                    
                    # Get user's saved rule or create default
                    saved_rule = user_rules.get(pattern_type, {})
                    
                    # Create display name
                    display_name = pattern_type.replace("_", " ").title()
                    
                    patterns.append({
                        "id": pattern_type,
                        "strategy_id": f"strategy_{pattern_type}",
                        "pattern_type": pattern_type,
                        "display_name": display_name,
                        "enabled": saved_rule.get("enabled", False),
                        "broker": saved_rule.get("broker"),
                        "max_capital_per_trade": saved_rule.get("max_capital_per_trade"),
                        "max_lots_per_trade": saved_rule.get("max_lots_per_trade"),
                        "daily_loss_limit": saved_rule.get("daily_loss_limit"),
                        "stop_loss_mode": saved_rule.get("stop_loss_mode", "FROM_ALERT"),
                        "custom_stop_loss_pct": saved_rule.get("custom_stop_loss_pct"),
                        "auto_execute": saved_rule.get("auto_execute", False)
                    })
    except Exception as e:
        print(f"Error loading pattern registry: {e}")
        # Return empty list if registry can't be loaded
        return []
    
    return patterns  # Return the list of strategy rules

@app.post("/api/auto_trade/rules")
@limiter.limit("30/minute")
async def save_auto_trade_rule(
    request: Request,
    rule: dict,
    current_user: dict = Depends(get_current_user)
):
    """Save auto-trade strategy rule for current user"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    # Get or create auto_trade_rules
    auto_trade_rules = user_data.get("auto_trade_rules", {})
    
    # Save the rule
    pattern_type = rule.get("pattern_type") or rule.get("id")
    if not pattern_type:
        raise HTTPException(status_code=400, detail="pattern_type or id is required")
    
    auto_trade_rules[pattern_type] = {
        "enabled": rule.get("enabled", False),
        "broker": rule.get("broker"),
        "max_capital_per_trade": rule.get("max_capital_per_trade"),
        "max_lots_per_trade": rule.get("max_lots_per_trade"),
        "daily_loss_limit": rule.get("daily_loss_limit"),
        "stop_loss_mode": rule.get("stop_loss_mode", "FROM_ALERT"),
        "custom_stop_loss_pct": rule.get("custom_stop_loss_pct"),
        "auto_execute": rule.get("auto_execute", False),
        "updated_at": datetime.now(timezone.utc).isoformat()
    }
    
    user_data["auto_trade_rules"] = auto_trade_rules
    
    from redis_files.redis_client import RedisClientFactory
    system_client = RedisClientFactory.get_system_client()
    system_client.set(f"user:{username}", json.dumps(user_data))
    
    # Return the saved rule in the expected format
    return {
        "id": pattern_type,
        "strategy_id": rule.get("strategy_id", f"strategy_{pattern_type}"),
        "pattern_type": pattern_type,
        "display_name": rule.get("display_name", pattern_type.replace("_", " ").title()),
        "enabled": auto_trade_rules[pattern_type]["enabled"],
        "broker": auto_trade_rules[pattern_type]["broker"],
        "max_capital_per_trade": auto_trade_rules[pattern_type]["max_capital_per_trade"],
        "max_lots_per_trade": auto_trade_rules[pattern_type]["max_lots_per_trade"],
        "daily_loss_limit": auto_trade_rules[pattern_type]["daily_loss_limit"],
        "stop_loss_mode": auto_trade_rules[pattern_type]["stop_loss_mode"],
        "custom_stop_loss_pct": auto_trade_rules[pattern_type]["custom_stop_loss_pct"],
        "auto_execute": auto_trade_rules[pattern_type]["auto_execute"]
    }

# ============================================================================
# PORTFOLIO ENDPOINTS
# ============================================================================

@app.get("/api/trades")
@limiter.limit("60/minute")
async def get_trades(
    request: Request,
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    current_user: dict = Depends(get_current_user)
):
    """Get trade history for the current user"""
    username = current_user.get("username")
    try:
        from redis_files.redis_client import RedisClientFactory
        system_client = RedisClientFactory.get_system_client()
        trade_keys = system_client.keys(f"trades:{username}:*")
        
        trades = []
        for key in trade_keys:
            trade_data = system_client.get(key)
            if trade_data:
                trades.append(json.loads(trade_data))
                
        # Sort by executed_at descending
        trades.sort(key=lambda x: x.get("executed_at", ""), reverse=True)
        
        total = len(trades)
        paginated_trades = trades[offset:offset+limit]
        
        return {
            "trades": paginated_trades,
            "total": total,
            "limit": limit,
            "offset": offset,
            "has_more": (offset + limit) < total
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ROBOT STATUS ENDPOINT
# ============================================================================

@app.get("/api/robot/status")
@limiter.limit("30/minute")
async def get_robot_status(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Get trading robot status (margin, positions, etc.)"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    # Get robot status from user data or return defaults
    robot_status = user_data.get("robot_status", {})
    
    # Default values if not set
    available_margin = robot_status.get("available_margin", 0)
    margin_used = robot_status.get("margin_used", 0)
    total_capital = robot_status.get("total_capital", available_margin + margin_used)
    safe_capital = robot_status.get("safe_capital", total_capital // 3)  # 1/3 rule
    open_positions = robot_status.get("open_positions", 0)
    margin_utilization_pct = (margin_used / total_capital * 100) if total_capital > 0 else 0
    
    return {
        "available_margin": available_margin,
        "margin_used": margin_used,
        "safe_capital": safe_capital,
        "open_positions": open_positions,
        "total_capital": total_capital,
        "margin_utilization_pct": margin_utilization_pct
    }

@app.post("/api/robot/status")
@limiter.limit("30/minute")
async def set_robot_status(
    request: Request,
    status: Dict,
    current_user: dict = Depends(get_current_user)
):
    """Set trading robot status (margin, positions, etc.)"""
    username = current_user.get("username")
    user_data = await get_user_from_redis_with_timeout(username)
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    # Update robot status
    robot_status = user_data.get("robot_status", {})
    robot_status.update(status)
    user_data["robot_status"] = robot_status
    
    # Save user data back to Redis
    from redis_files.redis_client import RedisClientFactory
    system_client = RedisClientFactory.get_system_client()
    system_client.set(f"user:{username}", json.dumps(user_data))
    
    return robot_status

# ============================================================================
# REST API ENDPOINTS FOR FRONTEND
# ============================================================================

@app.get("/api/alerts")
@limiter.limit("100/minute")  # Rate limit: 100 requests per minute
async def get_alerts(
    request: Request,
    current_user: dict = Depends(get_current_user),  # Require JWT authentication
    symbol: Optional[str] = Query(None),
    pattern: Optional[str] = Query(None),
    min_confidence: Optional[float] = Query(None),
    date_from: Optional[str] = Query(None),
    date_to: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0)
):
    """Get paginated alerts from Redis DB 1 (alerts:stream)"""
    try:
        from redis_files.redis_client import RedisClientFactory
        
        # Get Redis client for DB 1 (realtime - where alerts:stream is)
        client = RedisClientFactory.get_trading_client()
        
        alerts = []
        stream_name = "alerts:stream"
        
        # Read from alerts:stream (DB 1)
        try:
            stream_length = client.xlen(stream_name)
            max_messages = min(stream_length, limit + offset)
            
            # If no alerts in Redis, use sample data
            if stream_length == 0:
                sample_alerts = generate_sample_alerts(limit + offset)
                return {
                    "alerts": sample_alerts[offset:offset + limit],
                    "total": len(sample_alerts),
                    "limit": limit,
                    "offset": offset,
                    "last_updated": datetime.now().isoformat()
                }
            
            if max_messages > 0:
                # Read messages in reverse order (newest first)
                stream_messages = client.xrevrange(stream_name, count=max_messages)
                
                for msg_id, msg_data in stream_messages:
                    try:
                        # Extract alert data
                        data_field = msg_data.get('data') or msg_data.get(b'data')
                        if not data_field:
                            continue
                        
                        # Parse JSON
                        if isinstance(data_field, bytes):
                            try:
                                import orjson
                                alert = orjson.loads(data_field)
                            except (ImportError, Exception):
                                alert = json.loads(data_field.decode('utf-8'))
                        else:
                            alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                        
                        # Generate alert_id for legacy alerts that don't have it
                        if 'alert_id' not in alert or not alert.get('alert_id'):
                            ts = alert.get('timestamp') or alert.get('timestamp_ms', int(time.time() * 1000))
                            sym = alert.get('symbol', 'UNKNOWN')
                            alert['alert_id'] = f"{sym}_{ts}"
                        
                        # Ensure required fields for frontend
                        if 'signal' not in alert:
                            alert['signal'] = alert.get('direction', alert.get('action', 'NEUTRAL'))
                        if 'pattern_label' not in alert:
                            alert['pattern_label'] = alert.get('pattern', 'Unknown Pattern')
                        if 'base_symbol' not in alert:
                            # Extract base symbol from full symbol (e.g., "NIFTY25DEC26000CE" -> "NIFTY")
                            symbol_str = alert.get('symbol', '')
                            if symbol_str:
                                # Try to extract base symbol (remove dates, strikes, option types)
                                import re
                                base_match = re.match(r'^([A-Z]+)', symbol_str)
                                alert['base_symbol'] = base_match.group(1) if base_match else symbol_str.split(':')[-1] if ':' in symbol_str else symbol_str
                        
                        # Apply filters
                        if symbol and alert.get('symbol') != symbol:
                            continue
                        if pattern and alert.get('pattern') != pattern:
                            continue
                        if min_confidence is not None and alert.get('confidence', 0) < min_confidence:
                            continue
                        
                        # Normalize timestamp format for frontend
                        alert_ts = alert.get('timestamp') or alert.get('timestamp_ms')
                        if alert_ts:
                            try:
                                # Convert timestamp to ISO format string for frontend
                                if isinstance(alert_ts, str):
                                    # Already ISO format or string, try to parse
                                    try:
                                        alert_dt = datetime.fromisoformat(alert_ts.replace('Z', '+00:00'))
                                        alert['timestamp'] = alert_dt.isoformat()
                                    except:
                                        # If parsing fails, try as number
                                        alert_ts_num = float(alert_ts)
                                        if alert_ts_num > 1e10:  # milliseconds
                                            alert_dt = datetime.fromtimestamp(alert_ts_num / 1000)
                                        else:  # seconds
                                            alert_dt = datetime.fromtimestamp(alert_ts_num)
                                        alert['timestamp'] = alert_dt.isoformat()
                                else:
                                    # Numeric timestamp - check if milliseconds or seconds
                                    alert_ts_num = float(alert_ts)
                                    if alert_ts_num > 1e10:  # milliseconds (timestamp > year 2001)
                                        alert_dt = datetime.fromtimestamp(alert_ts_num / 1000)
                                    else:  # seconds
                                        alert_dt = datetime.fromtimestamp(alert_ts_num)
                                    alert['timestamp'] = alert_dt.isoformat()
                                    alert['timestamp_ms'] = int(alert_ts_num if alert_ts_num > 1e10 else alert_ts_num * 1000)
                            except Exception as e:
                                # If normalization fails, use current time
                                alert['timestamp'] = datetime.now().isoformat()
                                print(f"Warning: Failed to normalize timestamp {alert_ts}: {e}")
                        
                        # Date filtering
                        if date_from or date_to:
                            alert_timestamp = alert.get('timestamp')
                            if alert_timestamp:
                                try:
                                    if isinstance(alert_timestamp, str):
                                        alert_dt = datetime.fromisoformat(alert_timestamp.replace('Z', '+00:00'))
                                    else:
                                        alert_ts_num = float(alert_timestamp)
                                        if alert_ts_num > 1e10:  # milliseconds
                                            alert_dt = datetime.fromtimestamp(alert_ts_num / 1000)
                                        else:  # seconds
                                            alert_dt = datetime.fromtimestamp(alert_ts_num)
                                    
                                    if date_from:
                                        from_dt = datetime.fromisoformat(date_from.replace('Z', '+00:00'))
                                        if alert_dt < from_dt:
                                            continue
                                    if date_to:
                                        to_dt = datetime.fromisoformat(date_to.replace('Z', '+00:00'))
                                        if alert_dt > to_dt:
                                            continue
                                except Exception:
                                    pass
                        
                        alerts.append(alert)
                    except Exception as e:
                        continue
        except Exception as e:
            print(f"Error reading alerts:stream: {e}")
        
        # ✅ ENRICH ALERTS WITH VALIDATION RESULTS: Update confidence from validation
        # Get DB 0 client for validation results
        try:
            from redis_files.redis_key_standards import RedisKeyStandards
            db0_client = RedisClientFactory.get_trading_client()
            
            for alert in alerts:
                alert_id = alert.get('alert_id')
                if not alert_id:
                    continue
                
                try:
                    validation = None
                    
                    # Try direct lookup first: forward_validation:alert:{alert_id}
                    validation_key = RedisKeyStandards.get_validation_key(alert_id)
                    validation_data = db0_client.get(validation_key)
                    
                    if validation_data:
                        # Parse validation data
                        if isinstance(validation_data, bytes):
                            validation_data = validation_data.decode('utf-8')
                        validation = json.loads(validation_data) if isinstance(validation_data, str) else validation_data
                    else:
                        # Fallback: Check forward_validation:results hash
                        hash_key = "forward_validation:results"
                        validation_data = db0_client.hget(hash_key, alert_id)
                        if validation_data:
                            if isinstance(validation_data, bytes):
                                validation_data = validation_data.decode('utf-8')
                            validation = json.loads(validation_data) if isinstance(validation_data, str) else validation_data
                    
                    if validation:
                        # Extract validation confidence score
                        validation_result = validation.get('validation_result', {})
                        if isinstance(validation_result, dict):
                            validation_confidence = validation_result.get('confidence_score')
                            if validation_confidence is not None:
                                # Store original confidence before updating
                                original_conf = alert.get('confidence', 0.0)
                                
                                # Update alert confidence with validation confidence
                                alert['confidence'] = float(validation_confidence)
                                alert['validation_confidence'] = float(validation_confidence)
                                alert['original_confidence'] = original_conf
                                alert['is_validated'] = True
                                
                                # Also add validation status
                                is_valid = validation_result.get('is_valid', False)
                                alert['validation_status'] = 'VALID' if is_valid else 'REJECTED'
                                
                                # Add validation metadata
                                alert['validation_metadata'] = {
                                    'is_valid': is_valid,
                                    'confidence_score': float(validation_confidence),
                                    'timestamp': validation.get('timestamp'),
                                    'reasons': validation_result.get('reasons', [])
                                }
                except Exception as e:
                    # Silently continue if validation lookup fails
                    pass
        except Exception as e:
            print(f"Warning: Failed to enrich alerts with validation results: {e}")
        
        # Apply pagination
        total = len(alerts)
        alerts = alerts[offset:offset+limit]
        
        return {
            "alerts": alerts,
            "total": total,
            "limit": limit,
            "offset": offset,
            "has_more": (offset + limit) < total
        }
    except Exception as e:
        print(f"Error in get_alerts: {e}")
        import traceback
        traceback.print_exc()
        return {"alerts": [], "total": 0, "limit": limit, "offset": offset, "has_more": False}


@app.get("/api/alerts/{alert_id}")
async def get_alert_by_id(alert_id: str):
    """Get single alert with full details from Redis DB 0 and DB 1"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_key_standards import RedisKeyStandards
        
        # Try DB 0 first (forward_validation:alert:{alert_id})
        db0_client = RedisClientFactory.get_trading_client()
        validation_key = RedisKeyStandards.get_validation_key(alert_id)
        
        alert_data = None
        try:
            data = db0_client.get(validation_key)
            if data:
                if isinstance(data, bytes):
                    alert_data = json.loads(data.decode('utf-8'))
                else:
                    alert_data = json.loads(data) if isinstance(data, str) else data
        except Exception:
            pass
        
        # If not found, try alerts:stream in DB 1
        if not alert_data:
            db1_client = RedisClientFactory.get_trading_client()
            try:
                # Search recent messages
                messages = db1_client.xrevrange("alerts:stream", count=1000)
                for msg_id, msg_data in messages:
                    data_field = msg_data.get('data') or msg_data.get(b'data')
                    if data_field:
                        try:
                            if isinstance(data_field, bytes):
                                import orjson
                                alert = orjson.loads(data_field)
                            else:
                                alert = json.loads(data_field)
                            
                            if alert.get('alert_id') == alert_id:
                                alert_data = alert
                                break
                        except Exception:
                            continue
            except Exception:
                pass
        
        if not alert_data:
            raise HTTPException(status_code=404, detail="Alert not found")
        
        # Get additional data (indicators, Greeks, volume profile, news, validation)
        symbol = alert_data.get('symbol', '')
        
        # ✅ UPDATED: Get indicators from DB 1 (unified, migrated from DB 5)
        indicators = {}
        if symbol:
            try:
                db1_client = RedisClientFactory.get_trading_client()
                # ✅ FIX: Use client directly - no wrapping needed
                wrapped = db1_client
                
                indicator_names = ['rsi', 'atr', 'vwap', 'ema_20', 'ema_50', 'macd', 'bollinger_bands']
                for ind_name in indicator_names:
                    key = f"indicators:{symbol}:{ind_name}"
                    data = wrapped.retrieve_by_data_type(key, "indicators_cache")
                    if data:
                        try:
                            if isinstance(data, bytes):
                                data = data.decode('utf-8')
                            indicators[ind_name] = json.loads(data) if isinstance(data, str) else data
                        except Exception:
                            indicators[ind_name] = data
            except Exception as e:
                print(f"Error loading indicators: {e}")
        
        # ✅ UPDATED: Get Greeks from DB 1 (unified, migrated from DB 5)
        greeks = {}
        if symbol:
            try:
                db1_client = RedisClientFactory.get_trading_client()
                # ✅ FIX: Use client directly - no wrapping needed
                wrapped = db1_client
                
                greeks_key = f"indicators:{symbol}:greeks"
                data = wrapped.retrieve_by_data_type(greeks_key, "indicators_cache")
                if data:
                    try:
                        if isinstance(data, bytes):
                            data = data.decode('utf-8')
                        greeks_data = json.loads(data) if isinstance(data, str) else data
                        if isinstance(greeks_data, dict) and 'value' in greeks_data:
                            greeks = greeks_data['value']
                        else:
                            greeks = greeks_data
                    except Exception:
                        pass
            except Exception as e:
                print(f"Error loading Greeks: {e}")
        
        # ✅ UPDATED: Get volume profile from DB 1 (unified, migrated from DB 2)
        volume_profile = {}
        if symbol:
            try:
                db1_client = RedisClientFactory.get_trading_client()
                from redis_files.redis_key_standards import RedisKeyStandards
                
                # ✅ Use canonical symbol for key construction
                canonical_symbol = RedisKeyStandards.canonical_symbol(symbol)
                poc_key = f"volume_profile:poc:{canonical_symbol}"
                poc_data = db1_client.hgetall(poc_key)
                
                if poc_data:
                    volume_profile = {
                        "poc_price": float(poc_data.get(b'poc_price', b'0').decode()) if isinstance(poc_data.get(b'poc_price'), bytes) else float(poc_data.get('poc_price', 0)),
                        "poc_volume": int(poc_data.get(b'poc_volume', b'0').decode()) if isinstance(poc_data.get(b'poc_volume'), bytes) else int(poc_data.get('poc_volume', 0)),
                        "value_area_high": float(poc_data.get(b'value_area_high', b'0').decode()) if isinstance(poc_data.get(b'value_area_high'), bytes) else float(poc_data.get('value_area_high', 0)),
                        "value_area_low": float(poc_data.get(b'value_area_low', b'0').decode()) if isinstance(poc_data.get(b'value_area_low'), bytes) else float(poc_data.get('value_area_low', 0)),
                        "profile_strength": float(poc_data.get(b'profile_strength', b'0').decode()) if isinstance(poc_data.get(b'profile_strength'), bytes) else float(poc_data.get('profile_strength', 0)),
                    }
            except Exception as e:
                print(f"Error loading volume profile: {e}")
        
        # Get chart data
        chart_data = await get_chart_data_internal(symbol)
        
        # Get news
        news = []
        if symbol:
            try:
                db1_client = RedisClientFactory.get_trading_client()
                news_key = f"news:latest:{symbol}"
                news_data = db1_client.get(news_key)
                if news_data:
                    if isinstance(news_data, bytes):
                        news_data = news_data.decode('utf-8')
                    news = json.loads(news_data) if isinstance(news_data, str) else news_data
                    if not isinstance(news, list):
                        news = [news] if news else []
            except Exception:
                pass
        
        # Get validation results from DB 0
        validation = {}
        try:
            validation_data = db0_client.get(validation_key)
            if validation_data:
                if isinstance(validation_data, bytes):
                    validation_data = validation_data.decode('utf-8')
                validation = json.loads(validation_data) if isinstance(validation_data, str) else validation_data
        except Exception:
            pass
        
        return {
            "alert": alert_data,
            "indicators": indicators,
            "greeks": greeks,
            "volume_profile": volume_profile,
            "chart_data": chart_data,
            "news": news,
            "validation": validation
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_alert_by_id: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/alerts/stats/summary")
@limiter.limit("100/minute")
async def get_alert_stats(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Get alerts summary with historical context"""
    try:
        from redis_files.redis_client import RedisClientFactory
        
        db1_client = RedisClientFactory.get_trading_client()
        
        # Read recent alerts from stream
        stream_length = db1_client.xlen("alerts:stream")
        messages = db1_client.xrevrange("alerts:stream", count=min(stream_length, 10000))
        
        # If no alerts in Redis, use sample data
        if stream_length == 0:
            sample_alerts = generate_sample_alerts(1000)
            today = datetime.now().date()
            today_alerts = [alert for alert in sample_alerts 
                           if datetime.fromisoformat(alert["timestamp"]).date() == today]
            
            # Calculate pattern frequency
            pattern_counts = {}
            for alert in sample_alerts[-100:]:  # Last 100 alerts
                pattern = alert["pattern"]
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
            
            top_pattern = max(pattern_counts.items(), key=lambda x: x[1])[0] if pattern_counts else "N/A"
            
            return {
                "total_alerts": len(sample_alerts),
                "today_alerts": len(today_alerts),
                "avg_confidence": round(sum(a["confidence"] for a in sample_alerts[-100:]) / min(100, len(sample_alerts)), 1),
                "top_pattern": top_pattern,
                "market_status": get_market_status(),
                "last_updated": datetime.now().isoformat(),
                "pattern_distribution": pattern_counts,
                "symbol_ranking": [],
                "confidence_distribution": {"high": 0, "medium": 0, "low": 0},
                "instrument_type_distribution": {},
                "news_enrichment_rate": 0.0
            }
        
        pattern_counts = {}
        symbol_counts = {}
        confidence_counts = {"high": 0, "medium": 0, "low": 0}
        instrument_type_counts = {}
        today_count = 0
        # Use IST timezone for "today" calculation
        from datetime import datetime, timezone, timedelta
        ist = timezone(timedelta(hours=5, minutes=30))
        today = datetime.now(ist).date()
        
        for msg_id, msg_data in messages:
            try:
                data_field = msg_data.get('data') or msg_data.get(b'data')
                if not data_field:
                    continue
                
                if isinstance(data_field, bytes):
                    try:
                        import orjson
                        alert = orjson.loads(data_field)
                    except:
                        alert = json.loads(data_field.decode('utf-8'))
                else:
                    alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                
                # Count patterns
                pattern = alert.get('pattern', 'other')
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                
                # Count symbols
                symbol = alert.get('symbol', 'UNKNOWN')
                base_symbol = symbol.split(':')[-1] if ':' in symbol else symbol
                symbol_counts[base_symbol] = symbol_counts.get(base_symbol, 0) + 1
                
                # Count confidence
                confidence = alert.get('confidence', 0)
                if confidence >= 0.8:
                    confidence_counts["high"] += 1
                elif confidence >= 0.5:
                    confidence_counts["medium"] += 1
                else:
                    confidence_counts["low"] += 1
                
                # Count instrument types
                inst_type = alert.get('instrument_type', 'UNKNOWN')
                instrument_type_counts[inst_type] = instrument_type_counts.get(inst_type, 0) + 1
                
                # Count today's alerts (IST timezone) - INSIDE the loop
                alert_ts = alert.get('timestamp')
                if alert_ts:
                    try:
                        if isinstance(alert_ts, str):
                            alert_dt = datetime.fromisoformat(alert_ts.replace('Z', '+00:00'))
                        else:
                            # Normalize timestamp - check if milliseconds or seconds
                            alert_ts_num = float(alert_ts)
                            if alert_ts_num > 1e10:  # milliseconds
                                alert_dt = datetime.fromtimestamp(alert_ts_num / 1000)
                            else:  # seconds
                                alert_dt = datetime.fromtimestamp(alert_ts_num)
                        
                        # Convert to IST (UTC+5:30)
                        alert_ist = alert_dt.replace(tzinfo=timezone.utc).astimezone(ist)
                        today_ist = datetime.now(ist).date()
                        
                        if alert_ist.date() == today_ist:
                            today_count += 1
                    except Exception:
                        pass
            except Exception:
                pass
        
        # Sort symbol ranking
        symbol_ranking = sorted(symbol_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # Calculate average confidence and top pattern
        total_confidence = 0
        confidence_count = 0
        for msg_id, msg_data in messages:
            try:
                data_field = msg_data.get('data') or msg_data.get(b'data')
                if not data_field:
                    continue
                
                if isinstance(data_field, bytes):
                    try:
                        import orjson
                        alert = orjson.loads(data_field)
                    except:
                        alert = json.loads(data_field.decode('utf-8'))
                else:
                    alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                
                conf = alert.get('confidence', 0)
                if conf > 0:
                    total_confidence += conf
                    confidence_count += 1
            except Exception:
                continue
        
        avg_confidence = total_confidence / confidence_count if confidence_count > 0 else 0.0
        top_pattern = max(pattern_counts.items(), key=lambda x: x[1])[0] if pattern_counts else "N/A"
        
        return {
            "total_alerts": stream_length,
            "today_alerts": today_count,
            "avg_confidence": avg_confidence,
            "top_pattern": top_pattern,
            "pattern_distribution": pattern_counts,
            "symbol_ranking": [{"symbol": s, "count": c} for s, c in symbol_ranking],
            "confidence_distribution": confidence_counts,
            "instrument_type_distribution": instrument_type_counts,
            "news_enrichment_rate": 0.75  # TODO: Calculate actual rate
        }
    except Exception as e:
        print(f"Error in get_alert_stats: {e}")
        return {
            "total_alerts": 0,
            "today_alerts": 0,
            "pattern_distribution": {},
            "symbol_ranking": [],
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0},
            "instrument_type_distribution": {},
            "news_enrichment_rate": 0.0
        }


@app.get("/api/indicators/{symbol}")
async def get_indicators(symbol: str, indicators: Optional[str] = Query(None)):
    """Get technical indicators for symbol from Redis DB 5 (indicators_cache)"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_client import RobustRedisClient
        
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # ✅ UPDATED: Get DB 1 client (unified, indicators migrated from DB 5)
        client = RedisClientFactory.get_trading_client()
        # ✅ FIX: Use client directly - no wrapping needed
        wrapped = client
        
        # Get requested indicators or all
        indicator_list = indicators.split(',') if indicators else [
            'rsi', 'atr', 'vwap', 'ema_5', 'ema_10', 'ema_20', 'ema_50', 'ema_100', 'ema_200',
            'macd', 'bollinger_bands', 'volume_ratio'
        ]
        
        result = {}
        for ind_name in indicator_list:
            ind_name = ind_name.strip()
            key = f"indicators:{symbol}:{ind_name}"
            try:
                data = wrapped.retrieve_by_data_type(key, "indicators_cache")
                if data:
                    try:
                        if isinstance(data, bytes):
                            data = data.decode('utf-8')
                        parsed = json.loads(data) if isinstance(data, str) else data
                        # Extract value if nested
                        if isinstance(parsed, dict) and 'value' in parsed:
                            result[ind_name] = parsed['value']
                        else:
                            result[ind_name] = parsed
                    except (json.JSONDecodeError, TypeError):
                        # Try as float
                        try:
                            result[ind_name] = float(data) if isinstance(data, (str, bytes)) else data
                        except (ValueError, TypeError):
                            result[ind_name] = data
            except Exception as e:
                continue
        
        return {
            "symbol": symbol,
            "timestamp": datetime.now().isoformat(),
            "indicators": result
        }
    except Exception as e:
        print(f"Error in get_indicators: {e}")
        import traceback
        traceback.print_exc()
        return {"symbol": symbol, "timestamp": datetime.now().isoformat(), "indicators": {}}


@app.get("/api/greeks/{symbol}")
async def get_greeks(symbol: str):
    """Get Greeks for symbol from Redis DB 5 (indicators_cache)"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_client import RobustRedisClient
        
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # ✅ UPDATED: Get DB 1 client (unified, indicators migrated from DB 5)
        realtime_client = RedisClientFactory.get_trading_client()
        
        key = f"indicators:{symbol}:greeks"
        data = realtime_client.get(key)
        
        if data:
            try:
                if isinstance(data, bytes):
                    data = data.decode('utf-8')
                parsed = json.loads(data) if isinstance(data, str) else data
                # Extract value if nested
                if isinstance(parsed, dict) and 'value' in parsed:
                    return parsed['value']
                return parsed
            except Exception as e:
                return {}
        
        return {}
    except Exception as e:
        print(f"Error in get_greeks: {e}")
        return {}


@app.get("/api/charts/{symbol}")
@limiter.limit("100/minute")
async def get_charts(
    request: Request,
    symbol: str,
    date_from: Optional[str] = Query(None),
    date_to: Optional[str] = Query(None),
    resolution: str = Query("5m"),
    include_indicators: bool = Query(False),
    period: str = Query("7d", description="Time period: 1d, 7d, 1w, 1m, 3m, 1y"),
    current_user: dict = Depends(get_current_user)
):
    """
    Get chart data with multiple fallback strategies.
    
    NEVER returns 500 error - always returns 200 with valid JSON (even if empty).
    This ensures frontend never breaks due to chart endpoint errors.
    """
    # Wrap entire function to ensure NEVER returns 500
    try:
        # Try to get data from Redis first
        try:
            redis_data = await get_chart_data_internal(symbol, date_from, date_to, resolution, include_indicators, period)
            
            # If no data from Redis or insufficient data (< 10 bars), use fallbacks
            redis_ohlc = redis_data.get("ohlc", [])
            if not redis_ohlc or len(redis_ohlc) < 10:
                # FALLBACK: Try to extract from alert data
                try:
                    ohlc_from_alerts = await extract_ohlc_from_alert_stream(symbol, limit=100)
                    if ohlc_from_alerts and len(ohlc_from_alerts) > 0:
                        indicators = {}
                        if include_indicators:
                            try:
                                indicators = calculate_technical_indicators(ohlc_from_alerts)
                            except:
                                pass
                        try:
                            market_status = get_market_status()
                        except:
                            market_status = "unknown"
                        return {
                            "symbol": symbol,
                            "ohlc": ohlc_from_alerts,
                            "indicators": indicators,
                            "indicators_overlay": indicators,
                            "market_status": market_status,
                            "data_type": "alert_extraction",
                            "period": period,
                            "count": len(ohlc_from_alerts)
                        }
                except Exception as alert_error:
                    print(f"Error extracting from alerts: {alert_error}")
                
                # FALLBACK: Try historical data
                try:
                    historical_data = await get_historical_data(symbol, period, current_user)
                    indicators = {}
                    if include_indicators and historical_data.get("data"):
                        try:
                            indicators = calculate_technical_indicators(historical_data["data"])
                        except:
                            pass
                    try:
                        market_status = get_market_status()
                        last_trading_day = get_last_trading_day().isoformat()
                    except:
                        market_status = "unknown"
                        last_trading_day = datetime.now().isoformat()
                    
                    return {
                        "symbol": symbol,
                        "ohlc": historical_data.get("data", []),
                        "indicators": indicators,
                        "indicators_overlay": indicators,
                        "market_status": market_status,
                        "last_trading_day": last_trading_day,
                        "data_type": "historical",
                        "period": period
                    }
                except Exception as hist_error:
                    print(f"Error in historical data: {hist_error}")
            else:
                # Redis data exists, but ensure indicators are calculated if requested
                if include_indicators and redis_data.get("ohlc") and len(redis_data.get("ohlc", [])) >= 20:
                    # Calculate indicators from Redis OHLC data if not already present
                    if not redis_data.get("indicators") or not redis_data.get("indicators_overlay"):
                        try:
                            indicators = calculate_technical_indicators(redis_data.get("ohlc", []))
                            redis_data["indicators"] = indicators
                            redis_data["indicators_overlay"] = indicators
                        except:
                            pass
                return redis_data
        except Exception as redis_error:
            print(f"Error in get_chart_data_internal: {redis_error}")
            # Continue to fallbacks below
        
        # If we get here, Redis data failed - try alert extraction
        try:
            ohlc_from_alerts = await extract_ohlc_from_alert_stream(symbol, limit=100)
            if ohlc_from_alerts and len(ohlc_from_alerts) > 0:
                indicators = {}
                if include_indicators:
                    try:
                        indicators = calculate_technical_indicators(ohlc_from_alerts)
                    except:
                        pass
                try:
                    market_status = get_market_status()
                except:
                    market_status = "unknown"
                return {
                    "symbol": symbol,
                    "ohlc": ohlc_from_alerts,
                    "indicators": indicators,
                    "indicators_overlay": indicators,
                    "market_status": market_status,
                    "data_type": "alert_extraction",
                    "period": period,
                    "count": len(ohlc_from_alerts)
                }
        except Exception as alert_error:
            print(f"Error extracting from alerts: {alert_error}")
        
        # FALLBACK: Try historical data
        try:
            historical_data = await get_historical_data(symbol, period, current_user)
            indicators = {}
            if include_indicators and historical_data.get("data"):
                try:
                    indicators = calculate_technical_indicators(historical_data["data"])
                except:
                    pass
            try:
                market_status = get_market_status()
                last_trading_day = get_last_trading_day().isoformat()
            except:
                market_status = "unknown"
                last_trading_day = datetime.now().isoformat()
            
            return {
                "symbol": symbol,
                "ohlc": historical_data.get("data", []),
                "indicators": indicators,
                "indicators_overlay": indicators,
                "market_status": market_status,
                "last_trading_day": last_trading_day,
                "data_type": "historical",
                "period": period
            }
        except Exception as fallback_error:
            print(f"Error in historical data fallback: {fallback_error}")
        
        # FINAL FALLBACK: Return empty but valid response (NEVER return 500)
        try:
            market_status = get_market_status()
        except:
            market_status = "unknown"
        
        return {
            "symbol": symbol,
            "ohlc": [],
            "indicators": {},
            "indicators_overlay": {},
            "market_status": market_status,
            "data_type": "empty",
            "period": period,
            "count": 0,
            "note": "Chart data temporarily unavailable - data loading"
        }
    except Exception as e:
        # CATCH-ALL: Ensure we NEVER return 500, even if something unexpected happens
        print(f"CRITICAL: Unexpected error in get_charts endpoint: {e}")
        import traceback
        traceback.print_exc()
        
        # Return empty but valid response
        return {
            "symbol": symbol,
            "ohlc": [],
            "indicators": {},
            "indicators_overlay": {},
            "market_status": "unknown",
            "data_type": "error_fallback",
            "period": period,
            "count": 0,
            "note": "Chart data temporarily unavailable"
        }


async def get_chart_data_internal(
    symbol: str,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    resolution: str = "5m",
    include_indicators: bool = False,
    period: str = "7d"
):
    """Internal function to get chart data"""
    try:
        from redis_files.redis_client import RedisClientFactory
        
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # Normalize symbol: extract index name from options contracts
        # Examples:
        # - "NFO:BANKNIFTY25DEC55400PE" -> "BANKNIFTY"
        # - "BANKNIFTY25DEC55400PE" -> "BANKNIFTY"
        # - "NSE:BANKNIFTY" -> "BANKNIFTY"
        # - "BANKNIFTY" -> "BANKNIFTY"
        def extract_index_symbol(s: str) -> str:
            # Remove exchange prefix
            cleaned = s.split(':')[-1] if ':' in s else s
            
            # Extract index name (NIFTY, BANKNIFTY, FINNIFTY, INDIA VIX)
            import re
            index_match = re.match(r'^(NIFTY|BANKNIFTY|FINNIFTY|INDIA VIX)', cleaned, re.IGNORECASE)
            if index_match:
                index_name = index_match.group(1).upper()
                # Check if there's more after index name (dates/strikes for options)
                after_index = cleaned[len(index_match.group(0)):].strip()
                if after_index and re.match(r'^\d', after_index):
                    # Has digits after index name - it's an options contract
                    # Return normalized index name
                    if index_name == 'NIFTY':
                        return 'NIFTY 50'
                    elif index_name == 'BANKNIFTY':
                        return 'NIFTY BANK'
                    else:
                        return index_name
                else:
                    # Pure index or index with standard suffix
                    if index_name == 'NIFTY' and not re.search(r'\s+50\s*$', cleaned, re.IGNORECASE):
                        return 'NIFTY 50'
                    elif index_name == 'BANKNIFTY' and not re.search(r'\s+BANK\s*$', cleaned, re.IGNORECASE):
                        return 'NIFTY BANK'
                    return cleaned.upper().strip()
            return cleaned
        
        # Check if symbol is an options/futures contract or pure index
        # If it's an options/futures contract, use it as-is (don't normalize)
        # Only normalize if we need to fall back to index data
        is_options_or_futures = False
        cleaned_for_check = symbol.split(':')[-1] if ':' in symbol else symbol
        import re
        # Check if it has digits after index name (options/futures format)
        index_match_check = re.match(r'^(NIFTY|BANKNIFTY|FINNIFTY|INDIA VIX)', cleaned_for_check, re.IGNORECASE)
        if index_match_check:
            after_index = cleaned_for_check[len(index_match_check.group(0)):].strip()
            if after_index and re.match(r'^\d', after_index):
                is_options_or_futures = True
        
        # If it's an options/futures contract, use the symbol as-is
        # Otherwise, normalize to index symbol
        if is_options_or_futures:
            normalized_symbol = symbol  # Keep original options/futures symbol
            base_symbol = extract_index_symbol(symbol)  # Extract base for fallback lookups
        else:
            normalized_symbol = extract_index_symbol(symbol)
            base_symbol = normalized_symbol
        
        # ✅ UPDATED: Get DB 1 client (unified, migrated from DB 2)
        db1_client = RedisClientFactory.get_trading_client()
        
        # Try ohlc_daily:{symbol} sorted set first (use original symbol for options/futures)
        # For options/futures, try the exact symbol first, then fall back to normalized index
        if is_options_or_futures:
            zset_key = f"ohlc_daily:{symbol}"  # Try exact options/futures symbol first
        else:
            zset_key = f"ohlc_daily:{normalized_symbol}"  # Use normalized for indices
        
        ohlc_data = []
        
        # For base indices (NIFTY, BANKNIFTY), try to get from index data if no historical data
        is_base_index = base_symbol in ['NIFTY', 'BANKNIFTY', 'NIFTY 50', 'NIFTY BANK', 'INDIA VIX']
        
        try:
            # Get all entries first
            zset_entries = db1_client.zrange(zset_key, 0, -1, withscores=True)
            
            if not zset_entries:
                # Try original symbol (in case it's stored with full format)
                zset_key = f"ohlc_daily:{symbol}"
                zset_entries = db1_client.zrange(zset_key, 0, -1, withscores=True)
            
            # ✅ Add alias key checks using standardized key builder patterns
            if not zset_entries:
                from redis_files.redis_key_standards import RedisKeyStandards, get_key_builder
                builder = get_key_builder()
                
                # Try timeseries format: ohlc:{symbol}:{interval}
                alias_keys = [
                    builder.live_ohlc_timeseries(normalized_symbol, resolution),  # e.g., ohlc:NIFTY 50:5m
                    builder.live_ohlc_timeseries(normalized_symbol, "1d"),        # Daily timeseries
                    builder.live_ohlc_timeseries(normalized_symbol, "1h"),        # Hourly timeseries
                    builder.live_ohlc_daily(normalized_symbol),                   # Standardized daily key
                    f"ohlc:{normalized_symbol}:{resolution}",                     # Direct format
                    f"ohlc:{normalized_symbol}:1d",                               # Daily direct format
                    f"ohlc:historical:{normalized_symbol}",                       # Historical format
                ]
                
                # If using original symbol for options/futures, also try those
                if is_options_or_futures and symbol != normalized_symbol:
                    alias_keys.extend([
                        builder.live_ohlc_timeseries(symbol, resolution),
                        builder.live_ohlc_timeseries(symbol, "1d"),
                        builder.live_ohlc_daily(symbol),
                        f"ohlc:{symbol}:{resolution}",
                        f"ohlc:{symbol}:1d",
                    ])
                
                # Try each alias key until we find data
                for alias_key in alias_keys:
                    zset_entries = db1_client.zrange(alias_key, 0, -1, withscores=True)
                    if zset_entries:
                        zset_key = alias_key  # Update key for later use
                        break
            
            # Filter to last 7 days if we have entries (for better performance and relevant data)
            if zset_entries:
                seven_days_ago_ms = int((time.time() - (7 * 24 * 60 * 60)) * 1000)
                filtered_entries = []
                for entry_data, timestamp_score in zset_entries:
                    timestamp_float = float(timestamp_score)
                    timestamp_ms = int(timestamp_float if timestamp_float > 1e10 else timestamp_float * 1000)
                    if timestamp_ms >= seven_days_ago_ms:
                        filtered_entries.append((entry_data, timestamp_score))
                # Use filtered entries if we have any, otherwise use all (might be less than 7 days)
                zset_entries = filtered_entries if filtered_entries else zset_entries
            
            # If still no data and it's a base index, try to get from index data
            if not zset_entries and is_base_index:
                # Try to get latest index data and create a minimal chart
                db1_client = RedisClientFactory.get_trading_client()
                
                # ✅ Updated index key map to match actual Redis keys
                index_key_map = {
                    'NIFTY': 'index:NSE:NIFTY 50',
                    'NIFTY 50': 'index:NSE:NIFTY 50',
                    'BANKNIFTY': 'index:NSE:NIFTY BANK',
                    'NIFTY BANK': 'index:NSE:NIFTY BANK',
                    'INDIA VIX': 'index:NSE:INDIA_VIX',  # Note: underscore in key
                }
                
                index_key = index_key_map.get(base_symbol, f"index:NSE:{base_symbol}")
                
                # Try multiple index key formats
                index_data = db1_client.get(index_key)
                if not index_data:
                    # Try alternative formats
                    alt_keys = [
                        f"index:NSE:{base_symbol.replace(' ', '_')}",  # NIFTY_50, NIFTY_BANK
                        f"index:NSE:{base_symbol.replace(' ', '')}",   # NIFTY50, NIFTYBANK
                        f"index:NSE:{base_symbol.upper()}",             # Uppercase
                    ]
                    for alt_key in alt_keys:
                        index_data = db1_client.get(alt_key)
                        if index_data:
                            index_key = alt_key
                            break
                
                if index_data:
                    if isinstance(index_data, bytes):
                        index_data = index_data.decode('utf-8')
                    try:
                        index_json = json.loads(index_data)
                        last_price = float(index_json.get('last_price', 0))
                        ohlc_obj = index_json.get('ohlc', {})
                        
                        if isinstance(ohlc_obj, dict) and ohlc_obj:
                            # Create a single OHLC bar from index data
                            timestamp_ms = int(time.time() * 1000)
                            ohlc_entry = {
                                "timestamp": timestamp_ms,
                                "open": float(ohlc_obj.get('open', last_price)),
                                "high": float(ohlc_obj.get('high', last_price)),
                                "low": float(ohlc_obj.get('low', last_price)),
                                "close": float(ohlc_obj.get('close', last_price)),
                                "volume": 0
                            }
                            ohlc_data.append(ohlc_entry)
                    except Exception as e:
                        print(f"Error creating chart from index data: {e}")
            
            for entry_data, timestamp_score in zset_entries:
                try:
                    if isinstance(entry_data, bytes):
                        entry_str = entry_data.decode('utf-8')
                    else:
                        entry_str = entry_data
                    
                    ohlc_json = json.loads(entry_str)
                    
                    # Convert timestamp
                    timestamp_float = float(timestamp_score)
                    if timestamp_float > 1e10:
                        timestamp_ms = int(timestamp_float)
                    else:
                        timestamp_ms = int(timestamp_float * 1000)
                    
                    ohlc_entry = {
                        "timestamp": timestamp_ms,
                        "open": float(ohlc_json.get('o', 0)),
                        "high": float(ohlc_json.get('h', 0)),
                        "low": float(ohlc_json.get('l', 0)),
                        "close": float(ohlc_json.get('c', 0)),
                        "volume": int(float(ohlc_json.get('v', 0)))
                    }
                    ohlc_data.append(ohlc_entry)
                except Exception:
                    continue
        except Exception as e:
            print(f"Error reading OHLC data: {e}")
        
        # ✅ UPDATED: If still no data, try ohlc_latest hash in DB 1 (unified)
        if len(ohlc_data) == 0:
            # Try normalized symbol first, then original
            for symbol_variant in [normalized_symbol, symbol, base_symbol]:
                ohlc_key = f"ohlc_latest:{symbol_variant}"
                ohlc_hash = db1_client.hgetall(ohlc_key)
                
                if ohlc_hash:
                    def decode_val(key, default=0):
                        val = ohlc_hash.get(key.encode() if isinstance(key, str) else key) or ohlc_hash.get(key, default)
                        if isinstance(val, bytes):
                            val = val.decode('utf-8')
                        try:
                            return float(val) if val else default
                        except (ValueError, TypeError):
                            return default
                    
                    # Bucket format: open, high, low, close, volume, timestamp
                    timestamp_ms = int(decode_val('timestamp', time.time() * 1000))
                    ohlc_entry = {
                        "timestamp": timestamp_ms,
                        "open": decode_val('open', 0),
                        "high": decode_val('high', 0),
                        "low": decode_val('low', 0),
                        "close": decode_val('close', 0),
                        "volume": int(decode_val('volume', 0))
                    }
                    ohlc_data.append(ohlc_entry)
                    break
        
        # If still no data and it's a base index, try to get from index data
        if len(ohlc_data) == 0 and is_base_index:
            # Try to get latest index data and create a minimal chart
            db1_client = RedisClientFactory.get_trading_client()
            # Map symbol names to Redis index keys
            index_key_map = {
                'NIFTY': 'index:NSE:NIFTY 50',
                'NIFTY 50': 'index:NSE:NIFTY 50',
                'BANKNIFTY': 'index:NSE:NIFTY BANK',
                'NIFTY BANK': 'index:NSE:NIFTY BANK',
                'INDIA VIX': 'index:NSEINDIA_VIX'  # ✅ FIXED: Use actual Redis key with underscore
            }
            index_key = index_key_map.get(base_symbol, f"index:NSE:{base_symbol}")
            index_data = db1_client.get(index_key)
            
            if index_data:
                if isinstance(index_data, bytes):
                    index_data = index_data.decode('utf-8')
                try:
                    index_json = json.loads(index_data)
                    last_price = float(index_json.get('last_price', 0))
                    ohlc_obj = index_json.get('ohlc', {})
                    
                    if isinstance(ohlc_obj, dict) and ohlc_obj:
                        # Create a single OHLC bar from index data
                        timestamp_ms = int(time.time() * 1000)
                        ohlc_entry = {
                            "timestamp": timestamp_ms,
                            "open": float(ohlc_obj.get('open', last_price)),
                            "high": float(ohlc_obj.get('high', last_price)),
                            "low": float(ohlc_obj.get('low', last_price)),
                            "close": float(ohlc_obj.get('close', last_price)),
                            "volume": 0
                        }
                        ohlc_data.append(ohlc_entry)
                except Exception as e:
                    print(f"Error creating chart from index data: {e}")
        
        # Sort by timestamp
        ohlc_data.sort(key=lambda x: x['timestamp'])
        
        # Filter by market hours (9:15 AM - 3:30 PM IST)
        def is_market_hours(timestamp_ms: int) -> bool:
            """Check if timestamp falls within Indian market hours (9:15 AM - 3:30 PM IST)"""
            try:
                # Convert timestamp (assumed to be in UTC) to IST
                # IST is UTC+5:30
                dt_utc = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
                ist_offset = timedelta(hours=5, minutes=30)
                dt_ist = dt_utc.replace(tzinfo=None) + ist_offset
                
                # Get time component in IST
                time_component = dt_ist.time()
                
                # Market hours: 9:15 AM to 3:30 PM IST
                market_open = dt_time(9, 15)  # 9:15 AM
                market_close = dt_time(15, 30)  # 3:30 PM
                
                return market_open <= time_component <= market_close
            except Exception as e:
                print(f"Error checking market hours: {e}")
                return True  # If parsing fails, include the data point
        
        # Filter out non-market hours data
        ohlc_data = [x for x in ohlc_data if is_market_hours(x['timestamp'])]
        
        # Calculate date range and filter to last 7 days if period is 7d
        if period == "7d" and ohlc_data:
            seven_days_ago_ms = int((time.time() - (7 * 24 * 60 * 60)) * 1000)
            ohlc_data = [x for x in ohlc_data if x['timestamp'] >= seven_days_ago_ms]
        
        # Apply date filters (if explicitly provided)
        if date_from:
            from_ts = int(datetime.fromisoformat(date_from.replace('Z', '+00:00')).timestamp() * 1000)
            ohlc_data = [x for x in ohlc_data if x['timestamp'] >= from_ts]
        if date_to:
            to_ts = int(datetime.fromisoformat(date_to.replace('Z', '+00:00')).timestamp() * 1000)
            ohlc_data = [x for x in ohlc_data if x['timestamp'] <= to_ts]
        
        # Calculate actual data range
        data_days = 0
        if ohlc_data:
            oldest_ts = min(x['timestamp'] for x in ohlc_data)
            newest_ts = max(x['timestamp'] for x in ohlc_data)
            data_days = (newest_ts - oldest_ts) / (1000 * 60 * 60 * 24)  # days
        
        # Get indicators overlay if requested
        indicators_overlay = {}
        if include_indicators and symbol and len(ohlc_data) > 0:
            try:
                # ✅ UPDATED: Get indicators from DB 1 (unified, migrated from DB 5)
                db1_indicators = RedisClientFactory.get_trading_client()
                
                # Fetch current indicator values
                ema_20 = db1_indicators.get(f"indicators:{symbol}:ema_20")
                ema_50 = db1_indicators.get(f"indicators:{symbol}:ema_50")
                ema_100 = db1_indicators.get(f"indicators:{symbol}:ema_100")
                ema_200 = db1_indicators.get(f"indicators:{symbol}:ema_200")
                vwap_val = db1_indicators.get(f"indicators:{symbol}:vwap")
                
                # Helper to parse indicator value
                def parse_indicator(val):
                    if not val:
                        return None
                    if isinstance(val, bytes):
                        val = val.decode('utf-8')
                    try:
                        return float(val)
                    except:
                        return None
                
                # Calculate EMA arrays (simple: use current EMA for all points, or calculate if we have historical)
                # For now, use current EMA values as constant lines (can be enhanced with historical calculation)
                ema_20_val = parse_indicator(ema_20)
                ema_50_val = parse_indicator(ema_50)
                ema_100_val = parse_indicator(ema_100)
                ema_200_val = parse_indicator(ema_200)
                vwap_val_parsed = parse_indicator(vwap_val)
                
                # Create arrays matching OHLC length
                if ema_20_val is not None:
                    indicators_overlay['ema_20'] = [ema_20_val] * len(ohlc_data)
                if ema_50_val is not None:
                    indicators_overlay['ema_50'] = [ema_50_val] * len(ohlc_data)
                if ema_100_val is not None:
                    indicators_overlay['ema_100'] = [ema_100_val] * len(ohlc_data)
                if ema_200_val is not None:
                    indicators_overlay['ema_200'] = [ema_200_val] * len(ohlc_data)
                if vwap_val_parsed is not None:
                    indicators_overlay['vwap'] = [vwap_val_parsed] * len(ohlc_data)
            except Exception as e:
                print(f"Error loading indicators overlay: {e}")
                import traceback
                traceback.print_exc()
        
        return {
            "symbol": symbol,
            "ohlc": ohlc_data,
            "indicators_overlay": indicators_overlay,
            "resolution": resolution,
            "count": len(ohlc_data),
            "data_days": round(data_days, 1) if data_days > 0 else 0,
            "data_range": {
                "oldest": oldest_ts if ohlc_data else None,
                "newest": newest_ts if ohlc_data else None,
            } if ohlc_data else {}
        }
    except Exception as e:
        print(f"Error in get_chart_data_internal: {e}")
        import traceback
        traceback.print_exc()
        # Return empty but valid response
        return {"symbol": symbol, "ohlc": [], "indicators_overlay": {}, "resolution": resolution, "count": 0}


async def extract_ohlc_from_alert_stream(symbol: str, limit: int = 100) -> List[Dict]:
    """
    Extract OHLC data from alerts:stream by reading recent alerts and extracting market_data.
    
    This is a fallback when separate ohlc_latest keys don't exist.
    Alerts contain complete market_data with time_buckets that can be converted to OHLC.
    """
    try:
        from redis_files.redis_client import RedisClientFactory
        import orjson
        
        # Get Redis client for DB 1 (where alerts:stream is)
        db1_client = RedisClientFactory.get_trading_client()
        
        # Normalize symbol for matching (extract base symbol from options contracts)
        def normalize_symbol_for_match(s: str) -> str:
            """Normalize symbol for matching - handles options contracts"""
            cleaned = s.split(':')[-1] if ':' in s else s
            # Extract base symbol (NIFTY, BANKNIFTY, etc.)
            import re
            index_match = re.match(r'^(NIFTY|BANKNIFTY|FINNIFTY|INDIA VIX)', cleaned, re.IGNORECASE)
            if index_match:
                base = index_match.group(1).upper()
                if base == 'NIFTY':
                    return 'NIFTY 50'
                elif base == 'BANKNIFTY':
                    return 'NIFTY BANK'
                return base
            return cleaned.upper()
        
        normalized_symbol = normalize_symbol_for_match(symbol)
        
        # Read recent alerts from stream
        stream_name = "alerts:stream"
        stream_length = db1_client.xlen(stream_name)
        
        if stream_length == 0:
            return []
        
        # Read recent messages (newest first)
        max_messages = min(stream_length, limit * 2)  # Read more to find matching symbols
        messages = db1_client.xrevrange(stream_name, count=max_messages)
        
        ohlc_data = []
        seen_timestamps = set()  # Avoid duplicates
        
        for msg_id, msg_data in messages:
            try:
                # Extract alert data
                data_field = msg_data.get('data') or msg_data.get(b'data')
                if not data_field:
                    continue
                
                # Parse alert JSON
                if isinstance(data_field, bytes):
                    try:
                        alert = orjson.loads(data_field)
                    except:
                        alert = json.loads(data_field.decode('utf-8'))
                else:
                    alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                
                # Check if symbol matches (exact or normalized)
                alert_symbol = alert.get('symbol', '')
                alert_base_symbol = alert.get('base_symbol', '')
                
                # Match if exact symbol or base symbol matches
                symbol_matches = (
                    alert_symbol == symbol or
                    alert_symbol == normalized_symbol or
                    alert_base_symbol == normalized_symbol or
                    normalize_symbol_for_match(alert_symbol) == normalized_symbol
                )
                
                if not symbol_matches:
                    continue
                
                # Extract market_data.session_data.time_buckets
                market_data = alert.get('market_data', {})
                session_data = market_data.get('session_data', {})
                time_buckets = session_data.get('time_buckets', {})
                
                if not time_buckets or not isinstance(time_buckets, dict):
                    continue
                
                # Convert time_buckets to OHLC format
                for bucket_key, bucket_data in time_buckets.items():
                    if not isinstance(bucket_data, dict):
                        continue
                    
                    # Extract timestamp from bucket
                    # Bucket key format: "HH:MM" or timestamp might be in bucket_data
                    timestamp_ms = None
                    
                    # Try to get timestamp from bucket_data
                    if 'last_timestamp' in bucket_data:
                        timestamp_ms = int(bucket_data['last_timestamp']) * 1000  # Convert to ms if seconds
                    elif 'timestamp' in bucket_data:
                        ts_val = bucket_data['timestamp']
                        if isinstance(ts_val, (int, float)):
                            timestamp_ms = int(ts_val if ts_val > 1e10 else ts_val * 1000)
                    elif 'first_timestamp' in bucket_data:
                        timestamp_ms = int(bucket_data['first_timestamp']) * 1000
                    
                    # If no timestamp in bucket, try to parse from bucket_key (HH:MM format)
                    if not timestamp_ms:
                        try:
                            # Parse "HH:MM" format and convert to today's timestamp
                            from datetime import datetime, time as dt_time
                            hour, minute = map(int, bucket_key.split(':'))
                            today = datetime.now().date()
                            bucket_dt = datetime.combine(today, dt_time(hour, minute))
                            timestamp_ms = int(bucket_dt.timestamp() * 1000)
                        except:
                            continue
                    
                    # Skip if we've seen this timestamp (avoid duplicates)
                    if timestamp_ms in seen_timestamps:
                        continue
                    seen_timestamps.add(timestamp_ms)
                    
                    # Extract OHLC values
                    open_price = float(bucket_data.get('open', bucket_data.get('last_price', 0)))
                    high_price = float(bucket_data.get('high', open_price))
                    low_price = float(bucket_data.get('low', open_price))
                    close_price = float(bucket_data.get('close', bucket_data.get('last_price', open_price)))
                    volume = int(bucket_data.get('bucket_incremental_volume', bucket_data.get('volume', 0)))
                    
                    # Only add if we have valid data
                    if open_price > 0 and close_price > 0:
                        ohlc_data.append({
                            "timestamp": timestamp_ms,
                            "open": open_price,
                            "high": high_price,
                            "low": low_price,
                            "close": close_price,
                            "volume": volume
                        })
                
            except Exception as e:
                print(f"Error processing alert for OHLC extraction: {e}")
                continue
        
        # Sort by timestamp (oldest first)
        ohlc_data.sort(key=lambda x: x['timestamp'])
        
        # Limit to requested number
        if len(ohlc_data) > limit:
            ohlc_data = ohlc_data[-limit:]
        
        return ohlc_data
        
    except Exception as e:
        print(f"Error in extract_ohlc_from_alert_stream: {e}")
        import traceback
        traceback.print_exc()
        return []


@app.get("/api/volume-profile/{symbol}")
@limiter.limit("100/minute")
async def get_volume_profile(
    request: Request,
    symbol: str,
    date: Optional[str] = Query(None),
    current_user: dict = Depends(get_current_user)
):
    """Get volume profile from Redis DB 2 (analytics)"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_key_standards import RedisKeyStandards
        
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # ✅ UPDATED: Get DB 1 client (unified, migrated from DB 2)
        client = RedisClientFactory.get_trading_client()
        
        # ✅ Use canonical symbol for key construction
        canonical_symbol = RedisKeyStandards.canonical_symbol(symbol)
        
        # Get POC data
        poc_key = f"volume_profile:poc:{canonical_symbol}"
        poc_data = client.hgetall(poc_key)
        
        result = {
            "poc_price": 0.0,
            "poc_volume": 0,
            "value_area_high": 0.0,
            "value_area_low": 0.0,
            "profile_strength": 0.0,
            "distribution": {}
        }
        
        if poc_data:
            # Decode bytes to values
            def decode_value(key, default=0):
                val = poc_data.get(key.encode() if isinstance(key, str) else key) or poc_data.get(key, default)
                if isinstance(val, bytes):
                    val = val.decode('utf-8')
                return val
            
            result["poc_price"] = float(decode_value('poc_price', 0))
            result["poc_volume"] = int(float(decode_value('poc_volume', 0)))
            result["value_area_high"] = float(decode_value('value_area_high', 0))
            result["value_area_low"] = float(decode_value('value_area_low', 0))
            result["profile_strength"] = float(decode_value('profile_strength', 0))
        
        # Get distribution if date provided
        if date:
            try:
                # ✅ Use canonical symbol for key construction
                dist_key = f"volume_profile:distribution:{canonical_symbol}:{date}"
                dist_data = client.hgetall(dist_key)
                if dist_data:
                    distribution = {}
                    for k, v in dist_data.items():
                        key_str = k.decode() if isinstance(k, bytes) else str(k)
                        val_str = v.decode() if isinstance(v, bytes) else str(v)
                        distribution[key_str] = int(float(val_str))
                    result["distribution"] = distribution
            except Exception:
                pass
        
        return result
    except Exception as e:
        print(f"Error in get_volume_profile: {e}")
        import traceback
        traceback.print_exc()
        return {
            "poc_price": 0.0,
            "poc_volume": 0,
            "value_area_high": 0.0,
            "value_area_low": 0.0,
            "profile_strength": 0.0,
            "distribution": {}
        }


@app.get("/api/news/{symbol}")
async def get_news_by_symbol(
    symbol: str,
    limit: int = Query(10, ge=1, le=100),
    hours_back: int = Query(24, ge=1, le=168)
):
    """Get news for symbol from Redis DB 1 (realtime)"""
    try:
        from redis_files.redis_client import RedisClientFactory
        
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # Get DB 1 client (realtime)
        client = RedisClientFactory.get_trading_client()
        
        news_key = f"news:latest:{symbol}"
        news_data = client.get(news_key)
        
        if news_data:
            try:
                if isinstance(news_data, bytes):
                    news_data = news_data.decode('utf-8')
                news_list = json.loads(news_data) if isinstance(news_data, str) else news_data
                if not isinstance(news_list, list):
                    news_list = [news_list] if news_list else []
                
                # Filter by hours_back
                cutoff_time = datetime.now() - timedelta(hours=hours_back)
                filtered_news = []
                for item in news_list:
                    try:
                        item_ts = item.get('timestamp')
                        if item_ts:
                            if isinstance(item_ts, str):
                                item_dt = datetime.fromisoformat(item_ts.replace('Z', '+00:00'))
                            else:
                                # Normalize timestamp - check if milliseconds or seconds
                                item_ts_num = float(item_ts)
                                if item_ts_num > 1e10:  # milliseconds
                                    item_dt = datetime.fromtimestamp(item_ts_num / 1000)
                                else:  # seconds
                                    item_dt = datetime.fromtimestamp(item_ts_num)
                            if item_dt >= cutoff_time:
                                filtered_news.append(item)
                    except Exception:
                        filtered_news.append(item)
                
                return filtered_news[:limit]
            except Exception as e:
                print(f"Error parsing news: {e}")
        
        return []
    except Exception as e:
        print(f"Error in get_news_by_symbol: {e}")
        return []


@app.get("/api/news/market/latest")
async def get_latest_market_news(limit: int = Query(25, ge=1, le=100)):
    """Get latest market news from Redis (DB 1) or file system fallback"""
    try:
        from redis_files.redis_client import RedisClientFactory
        
        # Try Redis DB 1 first (where gift_nifty_gap.py stores news)
        db1_client = RedisClientFactory.get_trading_client()
        
        # Try to get news from Redis - check multiple key patterns
        news_items = []
        
        # Pattern 1: news:MARKET_NEWS:* keys (used by gift_nifty_gap.py)
        try:
            market_news_keys = db1_client.keys("news:MARKET_NEWS:*")
            if market_news_keys:
                # Sort by timestamp (extract from key) and get latest
                market_news_keys.sort(reverse=True)  # Most recent first
                for key in market_news_keys[:limit]:
                    try:
                        data = db1_client.get(key)
                        if data:
                            if isinstance(data, bytes):
                                data = data.decode('utf-8')
                            news_item = json.loads(data) if isinstance(data, str) else data
                            news_items.append(news_item)
                    except Exception:
                        continue
        except Exception as e:
            print(f"Error reading news:MARKET_NEWS keys: {e}")
        
        # Pattern 2: news:item:YYYYMMDD:* keys (used by gift_nifty_gap.py)
        if len(news_items) < limit:
            try:
                from datetime import datetime
                today = datetime.now().strftime('%Y%m%d')
                item_keys = db1_client.keys(f"news:item:{today}:*")
                if item_keys:
                    for key in item_keys[:limit - len(news_items)]:
                        try:
                            data = db1_client.get(key)
                            if data:
                                if isinstance(data, bytes):
                                    data = data.decode('utf-8')
                                news_item = json.loads(data) if isinstance(data, str) else data
                                # Avoid duplicates
                                if not any(item.get('title') == news_item.get('title') for item in news_items):
                                    news_items.append(news_item)
                        except Exception:
                            continue
            except Exception as e:
                print(f"Error reading news:item keys: {e}")
        
        # Pattern 3: Check market_data.news:latest summary
        if len(news_items) == 0:
            try:
                summary_data = db1_client.get('market_data.news:latest')
                if summary_data:
                    if isinstance(summary_data, bytes):
                        summary_data = summary_data.decode('utf-8')
                    summary = json.loads(summary_data) if isinstance(summary_data, str) else summary_data
                    # Summary might contain news items or references
                    if isinstance(summary, dict) and 'items' in summary:
                        news_items = summary['items'][:limit]
            except Exception:
                pass
        
        # Sort news items by timestamp (newest first) before returning
        if news_items:
            def get_timestamp(item):
                """Extract timestamp from news item, return as datetime for sorting"""
                ts = item.get('timestamp') or item.get('collected_at') or item.get('written_at') or item.get('published_at') or item.get('date')
                if not ts:
                    return datetime.min  # Put items without timestamp at the end
                try:
                    if isinstance(ts, str):
                        # Try ISO format first
                        try:
                            dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                            return dt
                        except:
                            # Try other formats
                            for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%S.%f"]:
                                try:
                                    dt = datetime.strptime(ts, fmt)
                                    return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt
                                except:
                                    continue
                    elif isinstance(ts, (int, float)):
                        # Unix timestamp (seconds or milliseconds)
                        if ts > 1e10:  # milliseconds
                            return datetime.fromtimestamp(ts / 1000, tz=timezone.utc)
                        else:  # seconds
                            return datetime.fromtimestamp(ts, tz=timezone.utc)
                except Exception as e:
                    print(f"Error parsing timestamp {ts}: {e}")
                return datetime.min  # Put unparseable items at the end
            
            # Sort by timestamp descending (newest first)
            news_items.sort(key=get_timestamp, reverse=True)
            print(f"✅ Returning {len(news_items)} news items from Redis (sorted newest first)")
            return news_items[:limit]
        
        # Fallback to file system
        print("⚠️ No news in Redis, falling back to file system")
        return load_news_from_files(limit)
        
    except Exception as e:
        print(f"Error getting news from Redis: {e}, falling back to files")
        import traceback
        traceback.print_exc()
        return load_news_from_files(limit)

@app.get("/api/market-data")
async def get_market_data(current_user: dict = Depends(get_current_user)):
    """Get market chart data"""
    try:
        # Mock data for testing - can be replaced with real data later
        return {
            "chart_data": [],
            "message": "Market data endpoint working"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard-stats")
async def get_dashboard_stats():
    """Get all dashboard statistics in one call"""
    try:
        # Get stats from existing endpoints
        from redis_files.redis_client import RedisClientFactory

        db1_client = RedisClientFactory.get_trading_client()
        stream_length = db1_client.xlen("alerts:stream")
        
        # Use IST timezone for "today" calculation
        ist = timezone(timedelta(hours=5, minutes=30))
        today = datetime.now(ist).date()
        
        # Count today's alerts
        today_count = 0
        messages = db1_client.xrevrange("alerts:stream", count=min(stream_length, 1000))
        for msg_id, msg_data in messages:
            try:
                data_field = msg_data.get('data') or msg_data.get(b'data')
                if not data_field:
                    continue
                
                if isinstance(data_field, bytes):
                    try:
                        import orjson
                        alert = orjson.loads(data_field)
                    except:
                        alert = json.loads(data_field.decode('utf-8'))
                else:
                    alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                
                # Check if alert is from today
                ts = alert.get('timestamp') or alert.get('timestamp_ms', 0)
                if ts:
                    try:
                        # Normalize timestamp - check if already in seconds or milliseconds
                        ts_num = float(ts)
                        if ts_num > 1e10:  # milliseconds
                            alert_date = datetime.fromtimestamp(ts_num / 1000, tz=ist).date()
                        else:  # seconds
                            alert_date = datetime.fromtimestamp(ts_num, tz=ist).date()
                        if alert_date == today:
                            today_count += 1
                    except Exception:
                        continue
            except Exception:
                continue
        
        # Get latest alerts (for chart symbol selection)
        alerts_list = []
        try:
            # Get most recent alerts (limit 10 for symbol selection)
            recent_messages = db1_client.xrevrange("alerts:stream", count=min(stream_length, 10))
            for msg_id, msg_data in recent_messages:
                try:
                    data_field = msg_data.get('data') or msg_data.get(b'data')
                    if not data_field:
                        continue
                    
                    if isinstance(data_field, bytes):
                        try:
                            import orjson
                            alert = orjson.loads(data_field)
                        except:
                            alert = json.loads(data_field.decode('utf-8'))
                    else:
                        alert = json.loads(data_field) if isinstance(data_field, str) else data_field
                    
                    alerts_list.append(alert)
                except Exception:
                    continue
        except Exception as e:
            print(f"Error fetching alerts for dashboard: {e}")
        
        # Get latest news from Redis (same as /api/news/market/latest)
        try:
            # Try Redis DB 1 first (where gift_nifty_gap.py stores news)
            news_items = []
            
            # Pattern 1: news:MARKET_NEWS:* keys
            try:
                market_news_keys = db1_client.keys("news:MARKET_NEWS:*")
                if market_news_keys:
                    market_news_keys.sort(reverse=True)  # Most recent first
                    for key in market_news_keys[:5]:
                        try:
                            data = db1_client.get(key)
                            if data:
                                if isinstance(data, bytes):
                                    data = data.decode('utf-8')
                                news_item = json.loads(data) if isinstance(data, str) else data
                                news_items.append(news_item)
                        except Exception:
                            continue
            except Exception as e:
                print(f"Error reading news:MARKET_NEWS keys: {e}")
            
            # Pattern 2: news:item:YYYYMMDD:* keys
            if len(news_items) < 5:
                try:
                    
                    today_str_format = datetime.now().strftime('%Y%m%d')
                    item_keys = db1_client.keys(f"news:item:{today_str_format}:*")
                    if item_keys:
                        for key in item_keys[:5 - len(news_items)]:
                            try:
                                data = db1_client.get(key)
                                if data:
                                    if isinstance(data, bytes):
                                        data = data.decode('utf-8')
                                    news_item = json.loads(data) if isinstance(data, str) else data
                                    if not any(item.get('title') == news_item.get('title') for item in news_items):
                                        news_items.append(news_item)
                            except Exception:
                                continue
                except Exception as e:
                    print(f"Error reading news:item keys: {e}")
            
            # Sort news by timestamp (newest first)
            if news_items:
                def get_timestamp(item):
                    """Extract timestamp from news item, return as datetime for sorting"""
                    ts = item.get('timestamp') or item.get('collected_at') or item.get('written_at') or item.get('published_at') or item.get('date')
                    if not ts:
                        return datetime.min
                    try:
                        if isinstance(ts, str):
                            try:
                                dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                                return dt
                            except:
                                for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%S.%f"]:
                                    try:
                                        dt = datetime.strptime(ts, fmt)
                                        return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt
                                    except:
                                        continue
                        elif isinstance(ts, (int, float)):
                            if ts > 1e10:  # milliseconds
                                return datetime.fromtimestamp(ts / 1000, tz=timezone.utc)
                            else:  # seconds
                                return datetime.fromtimestamp(ts, tz=timezone.utc)
                    except Exception:
                        pass
                    return datetime.min
                
                news_items.sort(key=get_timestamp, reverse=True)
            
            # Fallback to files if no Redis news
            if not news_items:
                news_items = load_news_from_files(5)
        except Exception as e:
            print(f"Error getting news for dashboard: {e}")
            news_items = load_news_from_files(5)
        
        # Calculate pattern stats
        pattern_counts = {}
        confidence_sum = 0.0
        confidence_count = 0
        for alert in alerts_list[:100]:  # Use first 100 for stats
            pattern = alert.get('pattern') or alert.get('pattern_type') or 'N/A'
            pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
            conf = alert.get('confidence', 0)
            if conf > 0:
                confidence_sum += conf
                confidence_count += 1
        
        avg_confidence = confidence_sum / confidence_count if confidence_count > 0 else 0.0
        top_pattern = max(pattern_counts.items(), key=lambda x: x[1])[0] if pattern_counts else "N/A"
        
        return {
            "total_alerts": stream_length,
            "today_alerts": today_count,
            "avg_confidence": round(avg_confidence, 2),
            "top_pattern": top_pattern,
            "alerts": alerts_list[:10],  # Return recent alerts for chart symbol selection
            "news": news_items[:5] if isinstance(news_items, list) else [],
            "status": "success"
        }
    except Exception as e:
        print(f"Error in get_dashboard_stats: {e}")
        import traceback
        traceback.print_exc()
        # Return fallback data on error
        today_str = datetime.now().strftime("%Y-%m-%d")
        return {
            "total_alerts": 0,
            "today_alerts": 0,
            "avg_confidence": 0,
            "top_pattern": "N/A",
            "alerts": [],
            "news": [{"title": "Market update", "content": "No recent news available", "date": today_str}],
            "status": "success"
        }

@app.get("/api/portfolio")
async def get_portfolio(current_user: dict = Depends(get_current_user)):
    """Get portfolio data for the current user."""
    try:
        username = current_user.get("username")
        if not username:
            raise HTTPException(status_code=401, detail="Invalid user")
        portfolio_manager = UserPortfolioManager(username)
        portfolio = await portfolio_manager.get_portfolio()
        return portfolio
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/validation/{alert_id}")
async def get_validation_by_alert_id(alert_id: str):
    """Get validation results for alert from Redis DB 0"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_key_standards import RedisKeyStandards
        
        # Get DB 0 client (system)
        client = RedisClientFactory.get_trading_client()
        
        validation_key = RedisKeyStandards.get_validation_key(alert_id)
        data = client.get(validation_key)
        
        if data:
            try:
                if isinstance(data, bytes):
                    data = data.decode('utf-8')
                validation = json.loads(data) if isinstance(data, str) else data
                return validation
            except Exception:
                pass
        
        return {}
    except Exception as e:
        print(f"Error in get_validation_by_alert_id: {e}")
        return {}


@app.get("/api/validation/stats")
async def get_validation_stats():
    """Get validation performance statistics from Redis DB 0"""
    try:
        from redis_files.redis_client import RedisClientFactory
        from redis_files.redis_key_standards import RedisKeyStandards
        
        # Get DB 0 client (system)
        client = RedisClientFactory.get_trading_client()
        
        # Get performance stats
        # ✅ Use DatabaseAwareKeyBuilder for key construction
        from redis_files.redis_key_standards import get_key_builder
        builder = get_key_builder()
        stats_key = builder.analytics_alert_performance_stats()
        stats_data = client.get(stats_key)
        
        if stats_data:
            try:
                if isinstance(stats_data, bytes):
                    stats_data = stats_data.decode('utf-8')
                return json.loads(stats_data) if isinstance(stats_data, str) else stats_data
            except Exception:
                pass
        
        return {
            "total_validations": 0,
            "success_rate": 0.0,
            "average_confidence": 0.0,
            "pattern_performance": {}
        }
    except Exception as e:
        print(f"Error in get_validation_stats: {e}")
        return {
            "total_validations": 0,
            "success_rate": 0.0,
            "average_confidence": 0.0,
            "pattern_performance": {}
        }


# Helper functions for historical data
def generate_historical_data(base_price: float, intervals: int, start_date: datetime, end_date: datetime, symbol: str = ""):
    """Generate realistic historical price data (only during market hours: 9:15 AM - 3:30 PM IST)"""
    data = []
    current_price = base_price
    current_time = start_date
    
    time_step = (end_date - start_date) / intervals if intervals > 0 else timedelta(hours=1)
    
    # Market hours: 9:15 AM to 3:30 PM IST
    market_open_hour = 9
    market_open_minute = 15
    market_close_hour = 15
    market_close_minute = 30
    
    for i in range(intervals):
        # Advance time
        current_time = start_date + (time_step * i)
        
        # Check if current time is within market hours (9:15 AM - 3:30 PM IST)
        # Note: Assuming start_date and current_time are in IST or UTC
        # We'll check the time component
        time_component = current_time.time()
        market_open_time = dt_time(market_open_hour, market_open_minute)
        market_close_time = dt_time(market_close_hour, market_close_minute)
        
        # Skip if outside market hours
        if not (market_open_time <= time_component <= market_close_time):
            continue
        
        # Simulate price movement
        change_percent = random.uniform(-0.5, 0.5)  # -0.5% to +0.5%
        current_price = current_price * (1 + change_percent / 100)
        
        # Ensure price doesn't go too crazy
        if "NIFTY" in symbol.upper():
            current_price = max(1000, min(50000, current_price))
        else:
            current_price = max(100, min(100000, current_price))
        
        # Generate OHLC data
        open_price = current_price * random.uniform(0.999, 1.001)
        high_price = max(open_price, current_price) * random.uniform(1.001, 1.005)
        low_price = min(open_price, current_price) * random.uniform(0.995, 0.999)
        close_price = current_price
        
        # Convert timestamp to milliseconds (Unix epoch)
        timestamp_ms = int(current_time.timestamp() * 1000)
        
        data.append({
            "timestamp": timestamp_ms,  # Use milliseconds, not ISO format
            "open": round(open_price, 2),
            "high": round(high_price, 2),
            "low": round(low_price, 2),
            "close": round(close_price, 2),
            "volume": random.randint(100000, 5000000)
        })
        
        current_time += time_step
    
    return data

def calculate_technical_indicators(data: List[Dict]):
    """Calculate basic technical indicators"""
    if not data:
        return {}
    
    closes = [item["close"] for item in data]
    
    # Simple Moving Averages
    sma_20 = sum(closes[-20:]) / min(20, len(closes)) if len(closes) >= 20 else sum(closes) / len(closes)
    sma_50 = sum(closes[-50:]) / min(50, len(closes)) if len(closes) >= 50 else None
    
    # RSI (simplified)
    if len(closes) > 1:
        gains = sum(max(0, closes[i] - closes[i-1]) for i in range(1, len(closes))) / len(closes)
        losses = sum(max(0, closes[i-1] - closes[i]) for i in range(1, len(closes))) / len(closes)
        rsi = 100 - (100 / (1 + (gains / losses if losses != 0 else 1)))
    else:
        rsi = 50.0
    
    return {
        "sma_20": round(sma_20, 2),
        "sma_50": round(sma_50, 2) if sma_50 else None,
        "rsi": round(rsi, 2),
        "current_price": closes[-1] if closes else 0,
        "price_change": round(closes[-1] - closes[0], 2) if len(closes) > 1 else 0,
        "price_change_percent": round(((closes[-1] - closes[0]) / closes[0]) * 100, 2) if len(closes) > 1 and closes[0] != 0 else 0
    }

def get_market_status():
    """Determine if market is open or closed"""
    now = datetime.now()
    # Indian market hours: 9:15 AM to 3:30 PM IST
    market_open = now.replace(hour=9, minute=15, second=0, microsecond=0)
    market_close = now.replace(hour=15, minute=30, second=0, microsecond=0)
    
    # Check if weekend
    if now.weekday() >= 5:  # Saturday or Sunday
        return "closed_weekend"
    
    # Check market hours
    if market_open <= now <= market_close:
        return "open"
    else:
        return "closed"

def get_last_trading_day():
    """Get the last trading day (skip weekends)"""
    today = datetime.now()
    if today.weekday() == 0:  # Monday
        return today - timedelta(days=3)  # Friday
    elif today.weekday() >= 5:  # Weekend
        return today - timedelta(days=2 if today.weekday() == 6 else 1)
    else:
        return today - timedelta(days=1)

def generate_sample_alerts(count: int):
    """Generate sample historical alerts for demonstration"""
    symbols = ["RELIANCE", "TCS", "INFY", "HDFC", "HDFCBANK", "ICICIBANK", "SBIN", "BHARTIARTL"]
    patterns = ["Bullish Engulfing", "Bearish Engulfing", "Morning Star", "Evening Star", 
                "Hammer", "Shooting Star", "Double Top", "Double Bottom"]
    
    alerts = []
    base_time = datetime.now() - timedelta(days=30)
    
    for i in range(count):
        symbol = random.choice(symbols)
        pattern = random.choice(patterns)
        confidence = random.randint(60, 95)
        
        alert_time = base_time + timedelta(hours=random.randint(1, 720))  # Random time in last 30 days
        
        alerts.append({
            "id": i + 1,
            "symbol": symbol,
            "pattern": pattern,
            "confidence": confidence,
            "timestamp": alert_time.isoformat(),
            "price": round(random.uniform(1000, 5000), 2),
            "volume": random.randint(10000, 1000000),
            "timeframe": random.choice(["5min", "15min", "1h", "1d"]),
            "status": random.choice(["active", "triggered", "expired"])
        })
    
    # Sort by timestamp descending (newest first)
    alerts.sort(key=lambda x: x["timestamp"], reverse=True)
    return alerts

@app.get("/api/historical/{symbol}")
async def get_historical_data(
    symbol: str,
    period: str = Query("1d", description="Time period: 1d, 1w, 1m, 3m, 1y"),
    resolution: str = Query("5m", description="Resolution: 1m, 5m, 15m, 30m, 1h, day"),
    current_user: dict = Depends(get_current_user)
):
    """Get historical market data for any symbol using internal fallback data."""
    try:
        # Decode symbol if URL encoded
        symbol = symbol.replace('%3A', ':').replace('%2F', '/')
        
        # Generate fallback data directly (no broker API)
        end_date = datetime.now()
        
        # Set start date based on period
        if period == "1d":
            start_date = end_date - timedelta(days=1)
            intervals = 390  # 6.5 hours * 60 minutes
        elif period == "7d":
            start_date = end_date - timedelta(days=7)
            intervals = 2730  # 7 days * 6.5 hours * 60 minutes / 5 minute bars
        elif period == "1w":
            start_date = end_date - timedelta(weeks=1)
            intervals = 35   # 7 days * 5 data points per day
        elif period == "1m":
            start_date = end_date - timedelta(days=30)
            intervals = 30   # 30 days
        elif period == "3m":
            start_date = end_date - timedelta(days=90)
            intervals = 90   # 90 days
        else:  # 1y
            start_date = end_date - timedelta(days=365)
            intervals = 52   # 52 weeks
        
        # Generate realistic price data
        base_price = 22000 if "NIFTY" in symbol.upper() else 50000
        data = generate_historical_data(base_price, intervals, start_date, end_date, symbol)
        
        return {
            "symbol": symbol,
            "period": period,
            "resolution": resolution,
            "data": data,
            "last_updated": datetime.now().isoformat(),
            "data_type": "fallback"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/market/indices")
async def get_market_indices():
    """Get market indices (NIFTY, BANKNIFTY, VIX) from Redis
    
    Per REDIS_STORAGE_SIGNATURE.md:
    - ohlc_latest:{symbol} is in DB 2 (analytics_data) - bucket format
    - index:NSE:{name} is in DB 1 (realtime) - gift_nifty_gap.py format
    
    Priority:
    1. ohlc_latest:{symbol} hash in DB 2 (bucket format when market is on)
    2. index:NSE:{name} JSON in DB 1 (gift_nifty_gap.py format when market is off)
    
    Optimized: Uses cached Redis connections to avoid connection overhead.
    """
    try:
        # ✅ UPDATED: Use cached Redis clients - all data in DB 1 (unified)
        db1_client = get_cached_redis_client(process_name="api", db=1, decode_responses=False)
        # db2_client removed - all data in DB 1 (unified)
        
        indices = {}
        # Map index names to symbol variations for bucket lookup
        index_symbol_map = {
            'NIFTY 50': ['NIFTY 50', 'NIFTY', 'NSE:NIFTY 50'],
            'NIFTY BANK': ['NIFTY BANK', 'BANKNIFTY', 'NSE:NIFTY BANK'],
            'INDIA VIX': ['INDIA VIX', 'VIX', 'NSE:INDIA VIX']
        }
        
        # Map index names to gift_nifty_gap.py Redis keys
        gift_nifty_key_map = {
            'NIFTY 50': 'index:NSE:NIFTY 50',
            'NIFTY BANK': 'index:NSE:NIFTY BANK',
            'INDIA VIX': 'index:NSEINDIA_VIX'  # ✅ FIXED: Use actual Redis key with underscore
        }
        
        for index_name, symbol_variations in index_symbol_map.items():
            try:
                found = False
                
                # ✅ UPDATED: PRIORITY 1: Try ohlc_latest hash in DB 1 (unified, bucket format - when market is on)
                for symbol_variant in symbol_variations:
                    ohlc_key = f"ohlc_latest:{symbol_variant}"
                    ohlc_data = db1_client.hgetall(ohlc_key)
                    
                    if ohlc_data:
                        def decode_val(key, default=0):
                            val = ohlc_data.get(key.encode() if isinstance(key, str) else key) or ohlc_data.get(key, default)
                            if isinstance(val, bytes):
                                val = val.decode('utf-8')
                            try:
                                return float(val) if val else default
                            except (ValueError, TypeError):
                                return default
                        
                        # Bucket format: open, high, low, close, volume, timestamp
                        # Note: bucket format uses 'close' as the current price, no 'last_price' field
                        close_price = decode_val('close', 0)
                        last_price = close_price  # In bucket format, close IS the last_price
                        
                        # prev_close: try to get from yesterday's data or use current close as fallback
                        prev_close = decode_val('prev_close', close_price)
                        
                        # Calculate change: last_price - prev_close
                        change = last_price - prev_close
                        change_pct = ((change / prev_close) * 100) if prev_close > 0 else 0.0
                        
                        indices[index_name] = {
                            "last_price": last_price,
                            "prev_close": prev_close,
                            "change": change,
                            "change_pct": change_pct
                        }
                        found = True
                        break
                
                # PRIORITY 2: Fallback to gift_nifty_gap.py format in DB 1 (when market is off)
                if not found:
                    redis_key = gift_nifty_key_map.get(index_name)
                    if redis_key:
                        index_data = db1_client.get(redis_key)
                        
                        if index_data:
                            if isinstance(index_data, bytes):
                                index_data = index_data.decode('utf-8')
                            
                            try:
                                index_json = json.loads(index_data)
                                last_price = float(index_json.get('last_price', 0))
                                
                                # Handle different data structures
                                ohlc = index_json.get('ohlc', {})
                                if isinstance(ohlc, dict):
                                    prev_close = float(ohlc.get('close', last_price))
                                else:
                                    prev_close = float(index_json.get('prev_close', last_price))
                                
                                # Calculate change from net_change or last_price - prev_close
                                net_change = index_json.get('net_change')
                                if net_change is not None:
                                    change = float(net_change)
                                else:
                                    change = float(index_json.get('change', last_price - prev_close))
                                
                                # Calculate change percentage
                                if prev_close > 0:
                                    change_pct = (change / prev_close) * 100
                                else:
                                    change_pct = 0.0
                                
                                indices[index_name] = {
                                    "last_price": last_price,
                                    "prev_close": prev_close,
                                    "change": change,
                                    "change_pct": change_pct
                                }
                                found = True
                            except (json.JSONDecodeError, ValueError, TypeError) as e:
                                print(f"Error parsing index data for {index_name}: {e}")
                
                # Default fallback if nothing found
                if not found:
                    indices[index_name] = {
                        "last_price": 0,
                        "prev_close": 0,
                        "change": 0,
                        "change_pct": 0
                    }
                    
            except Exception as e:
                print(f"Error fetching index {index_name}: {e}")
                import traceback
                traceback.print_exc()
                indices[index_name] = {
                    "last_price": 0,
                    "prev_close": 0,
                    "change": 0,
                    "change_pct": 0
                }
        
        # Add Gift Nifty gap data
        try:
            gap_data = db1_client.get('latest_gift_nifty_gap')
            if gap_data:
                if isinstance(gap_data, bytes):
                    gap_data = gap_data.decode('utf-8')
                gap_json = json.loads(gap_data)
                indices['GIFT_NIFTY_GAP'] = {
                    "gap_points": float(gap_json.get('gap_points', 0)),
                    "gap_percent": float(gap_json.get('gap_percent', 0)),
                    "gift_price": float(gap_json.get('gift_price', 0)),
                    "nifty_price": float(gap_json.get('nifty_price', 0)),
                    "signal": gap_json.get('signal', '')
                }
        except Exception as e:
            print(f"Error fetching Gift Nifty gap: {e}")
        
        return indices
    except Exception as e:
        print(f"Error in get_market_indices: {e}")
        import traceback
        traceback.print_exc()
        return {
            "NIFTY 50": {"last_price": 0, "prev_close": 0, "change": 0, "change_pct": 0},
            "NIFTY BANK": {"last_price": 0, "prev_close": 0, "change": 0, "change_pct": 0},
            "INDIA VIX": {"last_price": 0, "prev_close": 0, "change": 0, "change_pct": 0}
        }


@app.get("/api/intraday/instruments")
async def get_intraday_crawler_instruments():
    """Get all instruments from intraday crawler with their names and metadata"""
    try:
        import json
        from pathlib import Path
        from crawlers.utils.instrument_mapper import InstrumentMapper
        
        # Load crawler config - try multiple possible locations
        possible_paths = [
            Path(project_root) / "zerodha_websocket" / "crawlers" / "binary_crawler1" / "binary_crawler1.json",
            Path(project_root) / "crawlers" / "binary_crawler1" / "binary_crawler1.json",
        ]
        
        crawler_config_path = None
        for path in possible_paths:
            if path.exists():
                crawler_config_path = path
                break
        
        if not crawler_config_path:
            return {"instruments": [], "error": f"Crawler config not found in any of: {[str(p) for p in possible_paths]}"}
        
        with open(crawler_config_path, 'r') as f:
            crawler_config = json.load(f)
        
        tokens = crawler_config.get("tokens", [])
        from crawlers.hot_token_mapper import get_hot_token_mapper
        mapper = get_hot_token_mapper()
        
        instruments = []
        for token_str in tokens:
            try:
                token = int(token_str) if isinstance(token_str, str) else token_str
                metadata = mapper.get_token_metadata(token)
                
                if metadata:
                    instruments.append({
                        "token": token,
                        "symbol": metadata.get("tradingsymbol") or metadata.get("symbol") or f"UNKNOWN_{token}",
                        "name": metadata.get("name", ""),
                        "exchange": metadata.get("exchange", "NSE"),
                        "instrument_type": metadata.get("instrument_type", ""),
                        "segment": metadata.get("segment", ""),
                    })
                else:
                    # Fallback if metadata not found
                    instruments.append({
                        "token": token,
                        "symbol": f"UNKNOWN_{token}",
                        "name": "",
                        "exchange": "NSE",
                        "instrument_type": "",
                        "segment": "",
                    })
            except (ValueError, TypeError) as e:
                continue
        
        return {
            "instruments": instruments,
            "total": len(instruments),
            "crawler_name": crawler_config.get("name", "binary_crawler1"),
            "breakdown": crawler_config.get("instrument_breakdown", {})
        }
    except Exception as e:
        print(f"Error fetching intraday instruments: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# SPA routing: Serve React frontend for all non-API routes (must be last)
if IS_PRODUCTION:
    @app.get("/{full_path:path}")
    async def serve_frontend(full_path: str):
        """Serve React frontend for all routes (SPA routing)"""
        # Skip API and WebSocket routes
        if full_path.startswith("api") or full_path.startswith("ws") or full_path.startswith("alerts/stream"):
            raise HTTPException(status_code=404, detail="Not found")
        
        # Serve static files if they exist
        file_path = FRONTEND_DIST_PATH / full_path
        if file_path.exists() and file_path.is_file():
            return FileResponse(file_path)
        
        # Default to index.html for SPA routing
        return FileResponse(FRONTEND_DIST_PATH / "index.html")
